{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seven320/air_pollutants/blob/main/FP077_076.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCnFX004iEpo"
      },
      "source": [
        "# Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OhibwrnFAQk",
        "outputId": "6d1c3d1b-838c-4225-b471-b26fedcea772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri May 27 19:00:22 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b09jV65gFc3Y",
        "outputId": "5a7c2c5d-7059-4e03-d392-b6497fa1feb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if COLAB:\n",
        "    from google.colab import drive\n",
        "    from google.colab import output\n",
        "    drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MvUvD3E2I6sj"
      },
      "outputs": [],
      "source": [
        "if COLAB:\n",
        "    !pip install transformers > /dev/null\n",
        "    !pip install einops > /dev/null\n",
        "    !pip install optuna > /dev/null\n",
        "    !pip install pyephem > /dev/null\n",
        "    # !pip install timm > /dev/null\n",
        "    # !pip install kaggle > /dev/null\n",
        "    # !pip install kaggle_datasets > /dev/null\n",
        "    # !pip install git+https://github.com/albumentations-team/albumentations\n",
        "    # !pip install tensorflow-determinism\n",
        "    !pip install -q iterative-stratification\n",
        "    !pip install python-Levenshtein > /dev/null\n",
        "    !pip install geopy > /dev/null\n",
        "    !pip install catboost\n",
        "\n",
        "    # for gpu\n",
        "    !git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "    %cd /content/LightGBM/\n",
        "    !mkdir build\n",
        "    !cmake -DUSE_GPU=1 \n",
        "    !make -j$(nproc)\n",
        "    !sudo apt-get -y install python-pip\n",
        "\n",
        "    !sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
        "    #pandasのエラーが出る場合は上記のコードからpandasを削除\n",
        "\n",
        "    !sudo -H pip install setuptools numpy scipy scikit-learn -U\n",
        "    %cd /content/LightGBM/python-package\n",
        "    !sudo python setup.py install --precompile\n",
        "\n",
        "    output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VX7ZxFBMH3mX"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import copy\n",
        "import warnings\n",
        "from requests import get\n",
        "from contextlib import contextmanager\n",
        "from typing import List, Optional, TypeVar, Type, Dict\n",
        "import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GroupKFold, train_test_split, KFold\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "# from einops.layers.torch import Rearrange, Reduce\n",
        "# import timm\n",
        "import Levenshtein\n",
        "import ephem\n",
        "from geopy.distance import geodesic\n",
        "import pickle\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
        "from typing import Optional, Tuple\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "import catboost as cb\n",
        "import lightgbm as lgb\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3TLiFn8H_V90"
      },
      "outputs": [],
      "source": [
        "warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAG9b0Tb_YdF",
        "outputId": "173ea018-6b3b-4233-d1e1-726cdb0c0cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP077-076\n"
          ]
        }
      ],
      "source": [
        "if COLAB:    \n",
        "    NOTEBOOK_NAME = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
        "    print(NOTEBOOK_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BiWCgj_XI4y2"
      },
      "outputs": [],
      "source": [
        "COMPE_NAME = \"air_pollutants\"\n",
        "BASE_DIR = f\"/content/drive/MyDrive/signate/{COMPE_NAME}\"\n",
        "\n",
        "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tUAtLXzhI-Xr"
      },
      "outputs": [],
      "source": [
        "if COLAB:\n",
        "    INPUT_DIR = Path(os.path.join(BASE_DIR ,f\"input\"))\n",
        "    INPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    SAVE_DIR = Path(os.path.join(BASE_DIR ,f\"models/{NOTEBOOK_NAME}\"))\n",
        "    SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    OOF_DIR = Path(os.path.join(BASE_DIR, f\"oof/{NOTEBOOK_NAME}\"))\n",
        "    OOF_DIR.mkdir(exist_ok=True, parents = True)\n",
        "\n",
        "    SUB_DIR = Path(os.path.join(BASE_DIR, f\"submission/{NOTEBOOK_NAME}\"))\n",
        "    SUB_DIR.mkdir(exist_ok=True, parents = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z50ynnGpI-a-",
        "outputId": "7e9cc208-daa8-41f1-8faf-b9ebf56fb19e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(195941, 54) (53509, 53) (53509, 2)\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(INPUT_DIR, \"test.csv\"))\n",
        "sample_df = pd.read_csv(os.path.join(INPUT_DIR, \"submit_sample.csv\"), names=[\"id\", \"predict\"])\n",
        "\n",
        "print(train_df.shape, test_df.shape, sample_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "1LOr4fgFo5Rw",
        "outputId": "126a1633-d0d1-494f-a3b0-40366dde40d6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            id  year  month  day        Country          City       lat  \\\n",
              "0            1  2019      1    1      Australia      Brisbane -27.46794   \n",
              "1            2  2019      1    1      Australia        Darwin -12.46113   \n",
              "2            3  2019      1    1      Australia     Melbourne -37.81400   \n",
              "3            4  2019      1    1      Australia     Newcastle -32.92953   \n",
              "4            5  2019      1    1      Australia         Perth -31.95224   \n",
              "...        ...   ...    ...  ...            ...           ...       ...   \n",
              "195936  195937  2021     12   31  United States  Jacksonville  30.33218   \n",
              "195937  195938  2021     12   31  United States     Las Vegas  36.17497   \n",
              "195938  195939  2021     12   31  United States     Milwaukee  43.03890   \n",
              "195939  195940  2021     12   31        Vietnam         Hanoi  21.02450   \n",
              "195940  195941  2021     12   31        Vietnam       Hạ Long  20.95045   \n",
              "\n",
              "              lon  co_cnt  co_min  ...  ws_min  ws_mid  ws_max  ws_var  \\\n",
              "0       153.02809      38   0.749  ...   0.241   1.088   3.101   1.983   \n",
              "1       130.84185      47   2.594  ...   0.828   3.473   7.396  10.411   \n",
              "2       144.96332      17   1.190  ...   0.000   2.107   8.089  15.719   \n",
              "3       151.78010      63   4.586  ...   0.284   0.503   3.592   2.485   \n",
              "4       115.86140      47   4.689  ...   0.500   0.755   3.396   1.937   \n",
              "...           ...     ...     ...  ...     ...     ...     ...     ...   \n",
              "195936  -81.65565      12   0.694  ...   2.195   2.710   6.125   3.757   \n",
              "195937 -115.13722      14   0.528  ...   1.002   2.974   6.861   8.354   \n",
              "195938  -87.90647     171   1.975  ...   0.994   1.087   2.578   0.612   \n",
              "195939  105.84117      31   2.613  ...   1.005   3.058   6.005   6.085   \n",
              "195940  107.07336      26   0.069  ...   0.190   2.775   3.412   2.528   \n",
              "\n",
              "        dew_cnt  dew_min  dew_mid  dew_max  dew_var  pm25_mid  \n",
              "0            17    7.671   10.358   15.112   13.424    19.901  \n",
              "1            62   21.324   23.813   24.221    2.021    13.741  \n",
              "2            22   10.309   13.133   15.422    6.355    25.918  \n",
              "3           116    7.146   10.685   13.344    9.417   174.370  \n",
              "4            93    1.091    3.277   12.272    4.109   167.063  \n",
              "...         ...      ...      ...      ...      ...       ...  \n",
              "195936       12   16.774   22.679   26.058   13.252    16.150  \n",
              "195937       12   10.432   14.741   15.827    7.078    16.895  \n",
              "195938       26    2.049    3.531    6.686    5.286    86.299  \n",
              "195939       51    1.922    7.443    7.716    4.642    36.523  \n",
              "195940       16    8.448   10.372   18.886   11.536    62.021  \n",
              "\n",
              "[195941 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24fecd8e-3ad0-4002-a1c0-cfeb12de835d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>co_cnt</th>\n",
              "      <th>co_min</th>\n",
              "      <th>...</th>\n",
              "      <th>ws_min</th>\n",
              "      <th>ws_mid</th>\n",
              "      <th>ws_max</th>\n",
              "      <th>ws_var</th>\n",
              "      <th>dew_cnt</th>\n",
              "      <th>dew_min</th>\n",
              "      <th>dew_mid</th>\n",
              "      <th>dew_max</th>\n",
              "      <th>dew_var</th>\n",
              "      <th>pm25_mid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Brisbane</td>\n",
              "      <td>-27.46794</td>\n",
              "      <td>153.02809</td>\n",
              "      <td>38</td>\n",
              "      <td>0.749</td>\n",
              "      <td>...</td>\n",
              "      <td>0.241</td>\n",
              "      <td>1.088</td>\n",
              "      <td>3.101</td>\n",
              "      <td>1.983</td>\n",
              "      <td>17</td>\n",
              "      <td>7.671</td>\n",
              "      <td>10.358</td>\n",
              "      <td>15.112</td>\n",
              "      <td>13.424</td>\n",
              "      <td>19.901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Darwin</td>\n",
              "      <td>-12.46113</td>\n",
              "      <td>130.84185</td>\n",
              "      <td>47</td>\n",
              "      <td>2.594</td>\n",
              "      <td>...</td>\n",
              "      <td>0.828</td>\n",
              "      <td>3.473</td>\n",
              "      <td>7.396</td>\n",
              "      <td>10.411</td>\n",
              "      <td>62</td>\n",
              "      <td>21.324</td>\n",
              "      <td>23.813</td>\n",
              "      <td>24.221</td>\n",
              "      <td>2.021</td>\n",
              "      <td>13.741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Melbourne</td>\n",
              "      <td>-37.81400</td>\n",
              "      <td>144.96332</td>\n",
              "      <td>17</td>\n",
              "      <td>1.190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2.107</td>\n",
              "      <td>8.089</td>\n",
              "      <td>15.719</td>\n",
              "      <td>22</td>\n",
              "      <td>10.309</td>\n",
              "      <td>13.133</td>\n",
              "      <td>15.422</td>\n",
              "      <td>6.355</td>\n",
              "      <td>25.918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Newcastle</td>\n",
              "      <td>-32.92953</td>\n",
              "      <td>151.78010</td>\n",
              "      <td>63</td>\n",
              "      <td>4.586</td>\n",
              "      <td>...</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.503</td>\n",
              "      <td>3.592</td>\n",
              "      <td>2.485</td>\n",
              "      <td>116</td>\n",
              "      <td>7.146</td>\n",
              "      <td>10.685</td>\n",
              "      <td>13.344</td>\n",
              "      <td>9.417</td>\n",
              "      <td>174.370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Perth</td>\n",
              "      <td>-31.95224</td>\n",
              "      <td>115.86140</td>\n",
              "      <td>47</td>\n",
              "      <td>4.689</td>\n",
              "      <td>...</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.755</td>\n",
              "      <td>3.396</td>\n",
              "      <td>1.937</td>\n",
              "      <td>93</td>\n",
              "      <td>1.091</td>\n",
              "      <td>3.277</td>\n",
              "      <td>12.272</td>\n",
              "      <td>4.109</td>\n",
              "      <td>167.063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195936</th>\n",
              "      <td>195937</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>United States</td>\n",
              "      <td>Jacksonville</td>\n",
              "      <td>30.33218</td>\n",
              "      <td>-81.65565</td>\n",
              "      <td>12</td>\n",
              "      <td>0.694</td>\n",
              "      <td>...</td>\n",
              "      <td>2.195</td>\n",
              "      <td>2.710</td>\n",
              "      <td>6.125</td>\n",
              "      <td>3.757</td>\n",
              "      <td>12</td>\n",
              "      <td>16.774</td>\n",
              "      <td>22.679</td>\n",
              "      <td>26.058</td>\n",
              "      <td>13.252</td>\n",
              "      <td>16.150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195937</th>\n",
              "      <td>195938</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>United States</td>\n",
              "      <td>Las Vegas</td>\n",
              "      <td>36.17497</td>\n",
              "      <td>-115.13722</td>\n",
              "      <td>14</td>\n",
              "      <td>0.528</td>\n",
              "      <td>...</td>\n",
              "      <td>1.002</td>\n",
              "      <td>2.974</td>\n",
              "      <td>6.861</td>\n",
              "      <td>8.354</td>\n",
              "      <td>12</td>\n",
              "      <td>10.432</td>\n",
              "      <td>14.741</td>\n",
              "      <td>15.827</td>\n",
              "      <td>7.078</td>\n",
              "      <td>16.895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195938</th>\n",
              "      <td>195939</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>United States</td>\n",
              "      <td>Milwaukee</td>\n",
              "      <td>43.03890</td>\n",
              "      <td>-87.90647</td>\n",
              "      <td>171</td>\n",
              "      <td>1.975</td>\n",
              "      <td>...</td>\n",
              "      <td>0.994</td>\n",
              "      <td>1.087</td>\n",
              "      <td>2.578</td>\n",
              "      <td>0.612</td>\n",
              "      <td>26</td>\n",
              "      <td>2.049</td>\n",
              "      <td>3.531</td>\n",
              "      <td>6.686</td>\n",
              "      <td>5.286</td>\n",
              "      <td>86.299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195939</th>\n",
              "      <td>195940</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>Hanoi</td>\n",
              "      <td>21.02450</td>\n",
              "      <td>105.84117</td>\n",
              "      <td>31</td>\n",
              "      <td>2.613</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005</td>\n",
              "      <td>3.058</td>\n",
              "      <td>6.005</td>\n",
              "      <td>6.085</td>\n",
              "      <td>51</td>\n",
              "      <td>1.922</td>\n",
              "      <td>7.443</td>\n",
              "      <td>7.716</td>\n",
              "      <td>4.642</td>\n",
              "      <td>36.523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195940</th>\n",
              "      <td>195941</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>Hạ Long</td>\n",
              "      <td>20.95045</td>\n",
              "      <td>107.07336</td>\n",
              "      <td>26</td>\n",
              "      <td>0.069</td>\n",
              "      <td>...</td>\n",
              "      <td>0.190</td>\n",
              "      <td>2.775</td>\n",
              "      <td>3.412</td>\n",
              "      <td>2.528</td>\n",
              "      <td>16</td>\n",
              "      <td>8.448</td>\n",
              "      <td>10.372</td>\n",
              "      <td>18.886</td>\n",
              "      <td>11.536</td>\n",
              "      <td>62.021</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>195941 rows × 54 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24fecd8e-3ad0-4002-a1c0-cfeb12de835d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-24fecd8e-3ad0-4002-a1c0-cfeb12de835d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-24fecd8e-3ad0-4002-a1c0-cfeb12de835d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pseude label"
      ],
      "metadata": {
        "id": "HKfecU3smXR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oof_df = pd.read_csv(os.path.join(BASE_DIR, \"oof\", \"FP046-043\", \"oof.csv\"))\n",
        "submit_df = pd.read_csv(os.path.join(BASE_DIR, \"submission\", \"FP046-043\", \"submission.csv\"), names = [\"id\", \"pm25_mid\"])"
      ],
      "metadata": {
        "id": "c3D7iHIomb6P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oof_df_ = oof_df.rename(columns = {\"oof\":\"pseude_pm25_mid\"})\n",
        "submit_df_ = submit_df.rename(columns = {\"pm25_mid\":\"pseude_pm25_mid\"})"
      ],
      "metadata": {
        "id": "ay9afCdnnMIx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJgrFC5tVZt3"
      },
      "source": [
        "## add city population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fD-VnMAxyd8F"
      },
      "outputs": [],
      "source": [
        "# population_df = pd.read_csv(os.path.join(INPUT_DIR, \"city_population.csv\"))\n",
        "# population_df[\"City\"] = population_df[\"Name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PyVhbHAGdwTx"
      },
      "outputs": [],
      "source": [
        "# # 同じ都市名かつ国名のものを削除する\n",
        "# population_df[\"CC\"] = population_df[\"Country\"] + population_df[\"City\"]\n",
        "# population_df[\"CC\"].value_counts()\n",
        "# for k, v in population_df[\"CC\"].value_counts().items():\n",
        "#     # print(k, v)\n",
        "#     if v > 1:\n",
        "#         print(f\"{k} の行を削除\")\n",
        "#         population_df = population_df[population_df[\"CC\"] != k]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GHh1_7_-Dtka"
      },
      "outputs": [],
      "source": [
        "# def df_to_country_dict(df: pd.DataFrame) -> Dict:\n",
        "#     country_city = {}\n",
        "\n",
        "#     for t_c in set(df[\"Country\"].tolist()):\n",
        "#         country_city[t_c] = []\n",
        "\n",
        "#     for country, city in zip(df[\"Country\"], df[\"City\"]):\n",
        "#         country_city[country].append(city)\n",
        "\n",
        "#     for k, v in country_city.items():\n",
        "#         country_city[k] = set(v)\n",
        "\n",
        "#     return country_city"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JR3hx22DD7zE"
      },
      "outputs": [],
      "source": [
        "# train_country_city = df_to_country_dict(pd.concat([train_df, test_df]))\n",
        "# p_country_city = df_to_country_dict(population_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1Y2yypTHJBdQ"
      },
      "outputs": [],
      "source": [
        "# def cal_leven(str1: str, str2: str) -> float:\n",
        "#     # レーベンシュタイン距離の取得\n",
        "#     lev_dist = Levenshtein.distance(str1, str2)\n",
        "#     # 標準化(長い方の文字列の長さで割る)\n",
        "#     divider = len(str1) if len(str1) > len(str2) else len(str2)\n",
        "#     lev_dist = lev_dist / divider\n",
        "#     # 指標を合わせる(0:完全不一致 → 1:完全一致)\n",
        "#     return 1 - lev_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ktKYCePw0-WV"
      },
      "outputs": [],
      "source": [
        "# city_pairs = {}\n",
        "\n",
        "# for t_k, t_values in train_country_city.items():\n",
        "#     for p_k, p_values in p_country_city.items():\n",
        "#         # countryが一致\n",
        "#         if t_k != p_k:\n",
        "#             continue\n",
        "#         for t_v in t_values:\n",
        "#             max_score = -1\n",
        "#             for p_v in p_values:\n",
        "#                 score = cal_leven(t_v, p_v)\n",
        "#                 if max_score < score:\n",
        "#                     pairs = [t_v, p_v]\n",
        "#                     max_score = score\n",
        "\n",
        "#             if max_score > 0.8:\n",
        "#                 city_pairs[pairs[1]] = pairs[0]\n",
        "# print(f\"pair cities:{len(city_pairs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "o7H-pU-SOcQR"
      },
      "outputs": [],
      "source": [
        "# a = population_df[\"City\"]\n",
        "# for key, value in city_pairs.items():\n",
        "#     a = copy.deepcopy(a.replace(key, value))\n",
        "\n",
        "# population_df[\"City\"] = a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "o5b40dm-zrPh"
      },
      "outputs": [],
      "source": [
        "# if not \"Prev\" in train_df.columns:\n",
        "#     train_df = train_df.merge(population_df[[\"rank\", \"City\", \"Country\", \"Population\", \"Prev\", \"Growth\"]], on = [\"City\", \"Country\"], how=\"left\")\n",
        "#     test_df = test_df.merge(population_df[[\"rank\", \"City\", \"Country\",  \"Population\", \"Prev\", \"Growth\"]], on = [\"City\", \"Country\"], how=\"left\")\n",
        "# assert len(test_df) == len(sample_df)\n",
        "# assert train_df.shape[0] == 195941"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## add foot print data"
      ],
      "metadata": {
        "id": "4tCj6yJZ3JUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# footprint_df = pd.read_csv(os.path.join(INPUT_DIR, \"footprint.csv\"))\n",
        "\n",
        "# footprints_cap = []\n",
        "# footprints_cap_w = []\n",
        "# for vs in footprint_df[\"Footprint/cap(tCO2)\"].str.split(\"&plusmn;\"):\n",
        "#     v1, v2 = vs\n",
        "#     footprints_cap.append(v1)\n",
        "#     footprints_cap_w.append(v2)\n",
        "\n",
        "# footprint_df[\"footprint_cap\"] = footprints_cap\n",
        "# footprint_df[\"footprint_cap_w\"] = footprints_cap_w\n",
        "\n",
        "# footprint_df[\"footprint_cap\"] = footprint_df[\"footprint_cap\"].astype(float)\n",
        "# footprint_df[\"footprint_cap_w\"] = footprint_df[\"footprint_cap_w\"].astype(float)\n",
        "\n",
        "# footprints = []\n",
        "# footprints_w = []\n",
        "# for vs in footprint_df[\"Footprint(MtCO2)\"].str.split(\"&plusmn;\"):\n",
        "#     v1, v2 = vs\n",
        "#     footprints.append(v1)\n",
        "#     footprints_w.append(v2)\n",
        "\n",
        "# footprint_df[\"footprint\"] = footprints\n",
        "# footprint_df[\"footprint_w\"] = footprints_w\n",
        "\n",
        "# footprint_df[\"footprint\"] = footprint_df[\"footprint\"].astype(float)\n",
        "# footprint_df[\"footprint_w\"] = footprint_df[\"footprint_w\"].astype(float)\n",
        "\n",
        "# footprint_df = footprint_df.drop(columns = [\"Footprint(MtCO2)\", \"Footprint/cap(tCO2)\"])\n",
        "# footprint_df = footprint_df.replace({\n",
        "#     \"SouthKorea\": 'South Korea', \n",
        "#     \"USA\": \"United States\",\n",
        "#     \"SouthAfrica\": \"South Africa\",\n",
        "#     \"UnitedKingdom\": \"United Kingdom\",})\n",
        "# footprint_df[\"City\"] = footprint_df[\"UrbanCluster\"]\n",
        "# footprint_df = footprint_df.drop(columns = [\"UrbanCluster\"])"
      ],
      "metadata": {
        "id": "ZwB9zGlu91r1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f_country_city = footprint_df[\"Country\"].unique()\n",
        "# all_country = pd.concat([train_df, test_df])[\"Country\"].unique()"
      ],
      "metadata": {
        "id": "k3YJwDxw-_KL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# f_country_city = df_to_country_dict(footprint_df)"
      ],
      "metadata": {
        "id": "-wdA9DgaC6Kn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# city_pairs2 = {}\n",
        "\n",
        "# for t_k, t_values in train_country_city.items():\n",
        "#     for f_k, f_values in f_country_city.items():\n",
        "#         # countryが一致\n",
        "#         if t_k != f_k:\n",
        "#             continue\n",
        "#         for t_v in t_values:\n",
        "#             max_score = -1\n",
        "#             for f_v in f_values:\n",
        "#                 score = cal_leven(t_v, f_v)\n",
        "#                 if max_score < score:\n",
        "#                     pairs = [t_v, f_v]\n",
        "#                     max_score = score\n",
        "\n",
        "#             if max_score > 0.8:\n",
        "#                 city_pairs2[pairs[1]] = pairs[0]\n",
        "# print(f\"pair cities:{len(city_pairs2)}\")"
      ],
      "metadata": {
        "id": "d_THKdus-tYf"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## unknowncity lat/lon → city name\n",
        "# position_master_df = pd.concat([train_df, test_df])[[\"lat\", \"lon\", \"Country\", \"City\"]].drop_duplicates().reset_index(drop=True)\n",
        "# for c in footprint_df[\"City\"]:\n",
        "#     if \"Unknowncityatlat\" in c:\n",
        "#         lat, lon = c.replace(\"Unknowncityatlat/lon\", \"\").split(\",\")\n",
        "#         for i in position_master_df.index:\n",
        "#             lat_master, lon_master, city = position_master_df.loc[i, [\"lat\", \"lon\", \"City\"]]\n",
        "#             if abs(float(lat) - lat_master) + abs(float(lon) - lon_master) < 2:\n",
        "#                 city_pairs2[c] = city\n",
        "#                 break"
      ],
      "metadata": {
        "id": "SIA-VsaeMyyT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = footprint_df[\"City\"]\n",
        "# for key, value in city_pairs2.items():\n",
        "#     a = copy.deepcopy(a.replace(key, value))\n",
        "\n",
        "# footprint_df[\"City\"] = a\n",
        "\n",
        "# ## 重複している都市を削除\n",
        "# footprint_df = footprint_df[~footprint_df.duplicated(subset = [\"Country\", \"City\"])]"
      ],
      "metadata": {
        "id": "hI707GzHFlxr"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if not \"1StdDev\" in train_df.columns:\n",
        "#     train_df = train_df.merge(footprint_df[[\"Population\", \"1StdDev\", \"Globalranking\", \"Domesticranking\", \"footprint_cap\", \"footprint_cap_w\", \"footprint\", \"footprint_w\", \"City\", \"Country\"]], on = [\"City\", \"Country\"], how=\"left\")\n",
        "#     test_df = test_df.merge(footprint_df[[\"Population\", \"1StdDev\", \"Globalranking\", \"Domesticranking\", \"footprint_cap\", \"footprint_cap_w\", \"footprint\", \"footprint_w\", \"City\", \"Country\"]], on = [\"City\", \"Country\"], how=\"left\")\n",
        "# assert len(test_df) == len(sample_df)\n",
        "# assert train_df.shape[0] == 195941"
      ],
      "metadata": {
        "id": "e8e9lQjW-ta6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCrY06JStxpl"
      },
      "source": [
        "## add target encode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not \"target_count\" in train_df.columns:\n",
        "    target_df = train_df[[\"Country\", \"pm25_mid\"]].groupby(\"Country\", as_index = 1).describe()\n",
        "\n",
        "    target_df = target_df.rename(columns = {\n",
        "        \"count\": \"target_count\",\n",
        "        \"mean\": \"target_mean\",\n",
        "        \"std\": \"target_std\",\n",
        "        \"25%\": \"target_25%\",\n",
        "        \"50%\": \"target_50%\",\n",
        "        \"75%\": \"target_75%\",\n",
        "        \"max\": \"target_max\"\n",
        "    }).reset_index()\n",
        "    train_df = train_df.merge(pd.concat([target_df[\"Country\"], target_df[\"pm25_mid\"]], axis = 1), on = \"Country\", how = \"left\")\n",
        "    test_df = test_df.merge(pd.concat([target_df[\"Country\"], target_df[\"pm25_mid\"]], axis = 1), on = \"Country\", how = \"left\")"
      ],
      "metadata": {
        "id": "RO7S4go8M58F"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### add days"
      ],
      "metadata": {
        "id": "OByEklY9ZuiU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxc2G2QEOhFc"
      },
      "source": [
        "## 都市を近い順に入力(上位15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mRB3_ctT21va"
      },
      "outputs": [],
      "source": [
        "use_city = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "21cTCBpO_uln"
      },
      "outputs": [],
      "source": [
        "# 全ての都市を抽出\n",
        "all_df = pd.concat([train_df, test_df])\n",
        "lat_lons = all_df[[\"City\", \"lat\", \"lon\"]].drop_duplicates()\n",
        "\n",
        "train_cities = train_df[\"City\"].unique()\n",
        "test_cities = test_df[\"City\"].unique()\n",
        "all_cities = all_df[\"City\"].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9123cf5a5bd447c1b03ef37de8112ade",
            "cb2f7fa06a5b46c588c0c35df0858648",
            "a066d6f2ffed4d009cf8256e28aeb03f",
            "d6ad98f8376843e0b6af5e9783097d2f",
            "13919d39bb244ca495f7ad91c508c787",
            "0df84c0506524f24acdcaad04bdeaf34",
            "fcd5c16e04ba4d66a411e00cfb57382e",
            "3e1a774f7d804e8c9d321a1d3c89fd18",
            "68c0d3dea7b6487bb531f663c2c00d72",
            "692d270a6dc74857b74056304a5c4079",
            "59385a249a6646109f545c0e3df1c543"
          ]
        },
        "id": "-9XM3_4_QHOg",
        "outputId": "c3cfcc2a-dbe9-410a-fa32-7f083b278316"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/302 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9123cf5a5bd447c1b03ef37de8112ade"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        " # 各都市ごとの距離を格納\n",
        " #\n",
        "#  nearest_cities = {\n",
        "#       City1 = {\n",
        "            # City2: 500km\n",
        "            # City3: 1000km\n",
        "# }\n",
        "# } \n",
        "\n",
        "nearest_cities = {}\n",
        "for i in tqdm(range(len(lat_lons))):\n",
        "    City1, lat1, lon1 = lat_lons.iloc[i]\n",
        "    pos1 = np.array([lat1, lon1])\n",
        "    \n",
        "    nearest_city = {}\n",
        "    for j in range(len(lat_lons)):\n",
        "        if i == j:\n",
        "            continue\n",
        "        \n",
        "        City2, lat2, lon2 = lat_lons.iloc[j]\n",
        "        pos2 = np.array([lat2, lon2])\n",
        "        dis = geodesic(pos1, pos2)\n",
        "        nearest_city[City2] = dis\n",
        "\n",
        "    nearest_cities[City1] = nearest_city"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_sort_cities = []\n",
        "\n",
        "for city1 in nearest_cities.keys():\n",
        "    nearest_sort_city = dict(sorted(nearest_cities[city1].items(), key = lambda item: item[1])).keys()\n",
        "    nearest_sort_cities.append([city1] + list(nearest_sort_city)[:use_city])"
      ],
      "metadata": {
        "id": "Rl4mUNhSd8pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_en_bycities_detail = train_df[[\"City\",\"year\", \"month\",\"day\", \"pm25_mid\", \"so2_mid\", \"no2_mid\", \"co_mid\", \"o3_mid\"]].groupby([\"City\", \"year\", \"month\", \"day\"], as_index = 0).agg(\"mean\").reset_index(drop=True)"
      ],
      "metadata": {
        "id": "XMU0-WMd1Bvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cities = pd.DataFrame(target_en_bycities_detail[\"City\"].unique(), columns = [\"City\"])\n",
        "cities = pd.DataFrame(all_df[\"City\"].unique(), columns=[\"City\"])\n",
        "year_month_day = target_en_bycities_detail.groupby([\"year\", \"month\", \"day\"], as_index=0).agg(\"mean\")[[\"year\", \"month\", \"day\"]]\n",
        "\n",
        "city_year_month_day = pd.merge(cities, year_month_day, how = \"cross\")"
      ],
      "metadata": {
        "id": "vgZ0PLKzTIhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_en_bycities = train_df[[\"City\",\"year\", \"month\", \"pm25_mid\", \"so2_mid\", \"no2_mid\", \"co_mid\", \"o3_mid\"]].groupby([\"City\", \"month\"], as_index = 0).agg(\"mean\").reset_index(drop=True)"
      ],
      "metadata": {
        "id": "bEqcqt_ZSAcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_en = pd.merge(city_year_month_day, target_en_bycities_detail, on = [\"City\", \"year\", \"month\", \"day\"], how = \"left\")\n",
        "target_en_fillna = pd.merge(target_en, target_en_bycities, on = [\"City\", \"year\", \"month\"], how = \"left\")\n",
        "target_en_fillna[\"pm25_mid\"] = target_en_fillna[\"pm25_mid_x\"].combine_first(target_en_fillna[\"pm25_mid_y\"])\n",
        "target_en_fillna = target_en_fillna.drop(columns = [\"pm25_mid_x\", \"pm25_mid_y\"])\n",
        "target_en_fillna[\"so2_mid\"] = target_en_fillna[\"so2_mid_x\"].combine_first(target_en_fillna[\"so2_mid_y\"])\n",
        "target_en_fillna[\"no2_mid\"] = target_en_fillna[\"no2_mid_x\"].combine_first(target_en_fillna[\"no2_mid_y\"])\n",
        "target_en_fillna[\"co_mid\"] = target_en_fillna[\"co_mid_x\"].combine_first(target_en_fillna[\"co_mid_y\"])\n",
        "target_en_fillna[\"o3_mid\"] = target_en_fillna[\"o3_mid_x\"].combine_first(target_en_fillna[\"o3_mid_y\"])\n",
        "target_en_fillna = target_en_fillna.drop(columns = [\"so2_mid_x\", \"so2_mid_y\"])\n",
        "target_en_fillna = target_en_fillna.drop(columns = [\"no2_mid_x\", \"no2_mid_y\"])\n",
        "target_en_fillna = target_en_fillna.drop(columns = [\"co_mid_x\", \"co_mid_y\"])\n",
        "target_en_fillna = target_en_fillna.drop(columns = [\"o3_mid_x\", \"o3_mid_y\"])"
      ],
      "metadata": {
        "id": "0aUiJgMfQii1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pab-ry5T83Xa"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "NUM_FOLDS = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO13XyUmI-dG"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "## Utility ##\n",
        "#############\n",
        "@contextmanager\n",
        "def timer(name: str):\n",
        "    t0 = time.time()\n",
        "    print(f\"[{name}] start\")\n",
        "    yield\n",
        "    print(f\"[{name}] done - elapsed {time.time() - t0:.2f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeHnNhCrJNvb"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    \"\"\"\n",
        "    Seeds basic parameters for reproductibility of results\n",
        "    \n",
        "    Arguments:\n",
        "        seed {int} -- Number of the seed\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZZhyY_iXJyQ"
      },
      "outputs": [],
      "source": [
        "train_city = train_df[\"City\"].unique().tolist()\n",
        "test_city = test_df[\"City\"].unique().tolist()\n",
        "\n",
        "train_city_cnt = len(train_city)\n",
        "test_city_cnt = len(test_city)\n",
        "\n",
        "print(f\"train city count: {train_city_cnt}\")\n",
        "print(f\"test city count: {test_city_cnt}\")\n",
        "\n",
        "print(f\"{len(set(train_city) & set(test_city))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAxJgCMWvzyq"
      },
      "outputs": [],
      "source": [
        "kf = GroupKFold(n_splits = NUM_FOLDS)\n",
        "\n",
        "if not \"kfold\" in train_df.columns:\n",
        "    folds = copy.deepcopy(train_df[[\"id\"]])\n",
        "    folds[\"kfold\"] = -1\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df, train_df[\"pm25_mid\"], train_df[\"City\"])):\n",
        "        print(f\"train_idx: {len(train_idx)}, valid_idx: {len(valid_idx)}\")\n",
        "        folds.loc[valid_idx, \"kfold\"] = fold\n",
        "\n",
        "    train_df = train_df.merge(folds[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgLI7LLc1szm"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckDad6r_AevZ"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from typing import List, Optional, TypeVar\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class AbstractFeatureTransformer:\n",
        "    def __init__(self):\n",
        "        self.name = self.__class__.__name__\n",
        "\n",
        "    def fit_transform(self, input_df: pd.DataFrame, y=None):\n",
        "        self.fit(input_df, y)\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def fit(self, input_df: pd.DataFrame, y=None):\n",
        "        pass\n",
        "\n",
        "    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        raise NotImplementedError\n",
        "\n",
        "Transformer = TypeVar(\"Transformer\", bound=AbstractFeatureTransformer)\n",
        "\n",
        "def extract_features(input_df: pd.DataFrame,\n",
        "                     transformers: List[Transformer],\n",
        "                     fit: bool = True,\n",
        "                     logger: Optional[logging.Logger] = None):\n",
        "    feature_dfs = []\n",
        "    for transformer in transformers:\n",
        "        # timerはブロックの実行時間を計測するユーティリティ\n",
        "        with timer(f\"Extract features with {transformer.name}\", logger):\n",
        "            if fit:\n",
        "                feature_dfs.append(transformer.fit_transform(input_df))\n",
        "            else:\n",
        "                feature_dfs.append(transformer.transform(input_df))\n",
        "    all_features = pd.concat(feature_dfs, axis=1)\n",
        "    return all_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrG0N8hZDsoR"
      },
      "outputs": [],
      "source": [
        "class Numericals(AbstractFeatureTransformer):\n",
        "    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        cols = [pd.api.types.is_numeric_dtype(dtype) for dtype in input_df.dtypes]\n",
        "        return input_df.loc[:, cols]\n",
        "\n",
        "class LabelEncoding(AbstractFeatureTransformer):\n",
        "    def __init__(self, columns: List[str]):\n",
        "        super().__init__()\n",
        "        self.le_columns = columns\n",
        "        self.encoders = {\n",
        "            column: LabelEncoder()\n",
        "            for column in self.le_columns\n",
        "        }\n",
        "        self.__is_fitted = False\n",
        "\n",
        "    def fit(self, input_df: pd.DataFrame, y: Optional[np.ndarray] = None):\n",
        "        \"\"\"\n",
        "        ラベルに変換する前に欠損値を埋める\n",
        "        \"\"\"\n",
        "        for column in self.le_columns:\n",
        "            self.encoders[column].fit(input_df[column].fillna(\"\"))\n",
        "        self.__is_fitted = True\n",
        "\n",
        "    def transform(\n",
        "        self, \n",
        "        input_df: pd.DataFrame, \n",
        "        y: Optional[np.ndarray] = None\n",
        "    ) -> pd.DataFrame:\n",
        "        assert self.__is_fitted, \"You need to call `fit` first.\"\n",
        "        encoded = {}\n",
        "        for column in self.le_columns:\n",
        "            encoded[column] = self.encoders[column].transform(\n",
        "                input_df[column].fillna(\"\"))\n",
        "        return pd.DataFrame(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_all(train_df:pd.DataFrame, test_df:pd.DataFrame)->pd.DataFrame:\n",
        "    all_df = pd.concat([train_df, test_df])\n",
        "\n",
        "    mids = [\"co_mid\", \"o3_mid\", \"so2_mid\", \"no2_mid\", \"temperature_mid\", \"humidity_mid\", \"pressure_mid\", \"ws_mid\", \"dew_mid\", \n",
        "            \"co_max\", \"o3_max\", \"so2_max\", \"no2_max\", \"temperature_max\", \"humidity_max\", \"pressure_max\", \"ws_max\", \"dew_max\"]\n",
        "\n",
        "    for i in range(len(mids)):\n",
        "        if f\"scaled_{mids[i]}\" in train_df.columns:\n",
        "            break\n",
        "        scaler = MinMaxScaler()\n",
        "        scaler.fit(all_df[[mids[i]]])\n",
        "        train_df[f\"scaled_{mids[i]}\"] = scaler.transform(train_df[[mids[i]]])\n",
        "        test_df[f\"scaled_{mids[i]}\"] = scaler.transform(test_df[[mids[i]]])\n",
        "        cnt += 1\n",
        "    print(\"min max scaler\")\n",
        "\n",
        "    return train_df, test_df"
      ],
      "metadata": {
        "id": "Y2f0rxIkZXEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNf23jZJLku0"
      },
      "outputs": [],
      "source": [
        "def preprocess(df:pd.DataFrame)->pd.DataFrame:\n",
        "    df_ = copy.deepcopy(df)\n",
        "\n",
        "    df_[\"season\"] = df[\"month\"] // 3\n",
        "\n",
        "    # 年月/年月日を追加\n",
        "    df_[\"year_month\"] = df_[\"year\"].astype(\"str\") + df_[\"month\"].astype(\"str\")\n",
        "    df_[\"year_month_day\"] = df_[\"year\"].astype(\"str\") + df_[\"month\"].astype(\"str\") + df_[\"day\"].astype(\"str\")\n",
        "    df_[\"month_day\"] = df_[\"month\"].astype(\"str\") + df_[\"day\"].astype(\"str\")\n",
        "\n",
        "    df_[\"year_season\"] = df_[\"year\"].astype(\"str\") + df_[\"season\"].astype(\"str\")\n",
        "\n",
        "    # 月日国/月日都市を追加\n",
        "    df_[\"month_day_country\"] = df_[\"month\"].astype(\"str\") + df_[\"day\"].astype(\"str\") + df_[\"Country\"].astype(\"str\") \n",
        "    df_[\"month_day_city\"] =  df_[\"month\"].astype(\"str\") + df_[\"day\"].astype(\"str\") + df_[\"City\"].astype(\"str\") \n",
        "\n",
        "    # 経度をcosに変換\n",
        "    cnt = 0\n",
        "\n",
        "    if not \"lon_cos\" in df_.columns:\n",
        "        df_[\"lon_cos\"] = np.cos(np.radians((df_[\"lon\"] + 180)))\n",
        "        df_[\"lon_sin\"] = np.sin(np.radians((df_[\"lon\"] + 180)))\n",
        "        df_[\"month_cos\"] = np.cos(np.radians(df_[\"month\"] / 12 * 360))\n",
        "        df_[\"month_sin\"] = np.sin(np.radians(df_[\"month\"] / 12 * 360))\n",
        "        print(\"add lon_cos, lon_sin, month_cos, month_sin\")\n",
        "        cnt += 4\n",
        "\n",
        "    # 四則演算\n",
        "\n",
        "    # 南半球を北半球のmonthに変更\n",
        "    if not \"month_world\" in df_.columns:\n",
        "        df_[\"month_world\"] = df_[\"month\"]\n",
        "        df_.loc[df_[\"lat\"] < 0, \"month_world\"] = df_[\"month\"] + 6\n",
        "        df_.loc[df_[\"month_world\"] > 12, \"month_world\"] = df_[\"month\"] - 6\n",
        "        print(\"add month_world\")\n",
        "        cnt += 1\n",
        "    # mid同士を4則\n",
        "    columns = df_.columns\n",
        "    mids = []\n",
        "    for c in columns:\n",
        "        if \"_mid\" in c and c != \"pm25_mid\":\n",
        "            mids.append(c)\n",
        "\n",
        "    # 曜日を追加\n",
        "    def _cal_sun_time(lat:float, lon:float, year:float, month:float, day: float)->float:\n",
        "        location = ephem.Observer()\n",
        "        location.lat = str(lat)\n",
        "        location.lon = str(lon)\n",
        "        sun = ephem.Sun()\n",
        "\n",
        "        location.date = datetime.date(year = int(year), month = int(month), day = int(day))\n",
        "        sun_time_tmp1 = ephem.localtime(location.next_rising(sun)) - ephem.localtime(location.next_setting(sun))\n",
        "        sun_time_tmp2 = ephem.localtime(location.next_setting(sun)) - ephem.localtime(location.next_rising(sun))\n",
        "\n",
        "        sun_time = max(sun_time_tmp1.seconds, sun_time_tmp2.seconds)\n",
        "\n",
        "        return sun_time\n",
        "    if not \"sun_time\" in df_.columns:\n",
        "        u_cols = [\"lat\", \"lon\", \"year\", \"month\", \"day\"]\n",
        "\n",
        "        df_[\"sun_time\"] = -1\n",
        "        df_[\"week\"] = -1\n",
        "\n",
        "        mini_df = df_[u_cols]\n",
        "\n",
        "        sun_time = np.zeros(len(df_))\n",
        "        weeks = np.zeros(len(df_))\n",
        "        for i in tqdm(range(len(mini_df))):\n",
        "            lat, lon, year, month, day = mini_df.iloc[i]\n",
        "            sun_time[i] = _cal_sun_time(lat, lon, year, month, day)\n",
        "\n",
        "            # 曜日\n",
        "            date = datetime.date(int(year), int(month), int(day))\n",
        "            weeks[i] = date.weekday()\n",
        "\n",
        "        df_[\"sun_time\"] = sun_time\n",
        "        df_[\"week\"] = weeks\n",
        "\n",
        "        print(\"add sun_time, weeks\")\n",
        "    cnt += 2\n",
        "\n",
        "    mids = [\"co_mid\", \"o3_mid\", \"so2_mid\", \"no2_mid\", \"temperature_mid\", \"humidity_mid\", \"pressure_mid\", \"ws_mid\", \"dew_mid\"]\n",
        "    for i in range(len(mids)):\n",
        "        for j in range(i):\n",
        "            mid0, mid1 = mids[i], mids[j]\n",
        "            df_[f\"{mid0}+{mid1}\"] = df_[mid0] + df_[mid1]\n",
        "\n",
        "            cnt += 1\n",
        "    print(\"add mids + cal\")\n",
        "    scaled_mids = [\"scaled_co_mid\", \"scaled_o3_mid\", \"scaled_so2_mid\", \"scaled_no2_mid\"]\n",
        "    for i in range(len(scaled_mids)):\n",
        "        for j in range(i):\n",
        "            mid0, mid1 = scaled_mids[i], scaled_mids[j]\n",
        "            df_[f\"{mid0}+{mid1}\"] = df_[mid0] + df_[mid1]\n",
        "            cnt += 1\n",
        "            for k in range(j):\n",
        "                mid2 = scaled_mids[k]\n",
        "                df_[f\"{mid0}+{mid1}+{mid2}\"] = df_[mid0] + df_[mid1] + df_[mid2]\n",
        "                cnt += 1\n",
        "                for l in range(k):\n",
        "                    mid3 = scaled_mids[l]\n",
        "                    df_[f\"{mid0}+{mid1}+{mid2}+{mid3}\"] = df_[mid0] + df_[mid1] + df_[mid2] + df_[mid3]\n",
        "                    cnt += 1\n",
        "    print(\"add scaled mids\")\n",
        "\n",
        "    maxs = [\"co_max\", \"o3_max\", \"so2_max\", \"no2_max\", \"temperature_max\", \"humidity_max\", \"pressure_max\", \"ws_max\", \"dew_max\"]\n",
        "    for i in range(len(maxs)):\n",
        "        for j in range(i):\n",
        "            max0, max1 = maxs[i], maxs[j]\n",
        "            df_[f\"{max0}+{max1}\"] = df_[max0] + df_[max1]\n",
        "\n",
        "    scaled_maxs = [\"scaled_co_max\", \"scaled_o3_max\", \"scaled_so2_max\", \"scaled_no2_max\"]\n",
        "    for i in range(len(scaled_maxs)):\n",
        "        for j in range(i):\n",
        "            max0, max1 = scaled_maxs[i], scaled_maxs[j]\n",
        "            df_[f\"{max0}+{max1}\"] = df_[max0] + df_[max1]\n",
        "            cnt += 1\n",
        "            for k in range(j):\n",
        "                max2 = scaled_maxs[k]\n",
        "                df_[f\"{max0}+{max1}+{max2}\"] = df_[max0] + df_[max1] + df_[max2]\n",
        "                cnt += 1\n",
        "                for l in range(k):\n",
        "                    max3 = scaled_maxs[l]\n",
        "                    df_[f\"{max0}+{max1}+{max2}+{max3}\"] = df_[max0] + df_[max1] + df_[max2] + df_[max3]\n",
        "                    cnt += 1\n",
        "    print(\"add scaled maxs\")\n",
        "\n",
        "    print(f\"add {cnt} columns\")\n",
        "\n",
        "    return df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YsXccMiCXvd"
      },
      "outputs": [],
      "source": [
        "class AbstractTreeModel:\n",
        "    def __init__(self, prediction_type=\"regression\"):\n",
        "        self.model = None\n",
        "        self.prediction_type = prediction_type\n",
        "\n",
        "    def train(self,\n",
        "              params: dict,\n",
        "              X_train: pd.DataFrame,\n",
        "              y_train: np.ndarray,\n",
        "              X_val: pd.DataFrame,\n",
        "              y_val: np.ndarray,\n",
        "              train_weights: Optional[np.ndarray] = None,\n",
        "              val_weights: Optional[np.ndarray] = None,\n",
        "              train_params: Optional[dict] = None):\n",
        "        if train_params is None:\n",
        "            train_params = {}\n",
        "\n",
        "        model = self._train(\n",
        "            params,\n",
        "            X_train, y_train,\n",
        "            X_val, y_val,\n",
        "            train_weights, val_weights,\n",
        "            train_params)\n",
        "        self.model = model\n",
        "        return self\n",
        "\n",
        "    def _train(self,\n",
        "               params,\n",
        "               X_train,\n",
        "               y_train,\n",
        "               X_val,\n",
        "               y_val,\n",
        "               train_weights,\n",
        "               val_weights,\n",
        "               train_params):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def feature_names_(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _check_if_trained(self):\n",
        "        assert self.model is not None, \"You need to train the model first\"\n",
        "\n",
        "class LGBModel(AbstractTreeModel):\n",
        "    def _train(self,\n",
        "               params,\n",
        "               X_train, y_train,\n",
        "               X_val, y_val,\n",
        "               train_weights, val_weights,\n",
        "               train_params):\n",
        "        trn_data = lgb.Dataset(X_train, y_train, weight=train_weights)\n",
        "        val_data = lgb.Dataset(X_val, y_val, weight=val_weights)\n",
        "        model = lgb.train(params=params,\n",
        "                          train_set=trn_data,\n",
        "                          valid_sets=[trn_data, val_data],\n",
        "                          **train_params,)\n",
        "                        #   verbose_eval=False)\n",
        "        return model\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        self._check_if_trained()\n",
        "        return self.model.predict(X, num_iteration=self.model.best_iteration)\n",
        "\n",
        "    @property\n",
        "    def feature_names_(self):\n",
        "        self._check_if_trained()\n",
        "        return self.model.feature_name()\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        self._check_if_trained()\n",
        "        return self.model.feature_importance(importance_type=\"gain\")\n",
        "\n",
        "class XGBModel(AbstractTreeModel):\n",
        "    def _train(self,\n",
        "               params,\n",
        "               X_train, y_train,\n",
        "               X_val, y_val,\n",
        "               train_weights, val_weights,\n",
        "               train_params):\n",
        "        model = XGBRegressor(**params,\n",
        "            )\n",
        "\n",
        "        model.fit(X_train, y_train, \n",
        "                  eval_set = [(X_val, y_val)], \n",
        "                  **train_params\n",
        "                  )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        self._check_if_trained()\n",
        "        return self.model.predict(X, self.model.get_best_iteraion())\n",
        "\n",
        "    @property\n",
        "    def feature_names_(self):\n",
        "        self._check_if_trained()\n",
        "        # return self.use_cols\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        self._check_if_trained()\n",
        "        return self.model.feature_importances_\n",
        "\n",
        "class CatBoostModel(AbstractTreeModel):\n",
        "    def _train(self,\n",
        "               params,\n",
        "               X_train, y_train,\n",
        "               X_val, y_val,\n",
        "               train_weights, val_weights,\n",
        "               train_params):\n",
        "        \n",
        "        \n",
        "        trn_data = cb.Pool(X_train, y_train)\n",
        "        val_data = cb.Pool(X_val, y_val)\n",
        "\n",
        "        model = CatBoostRegressor(**params)\n",
        "        model.fit(X_train,\n",
        "                  y_train,\n",
        "                  eval_set = val_data,\n",
        "                  **train_params\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        self._check_if_trained()\n",
        "        return self.model.predict(X)\n",
        "\n",
        "    @property\n",
        "    def feature_names_(self):\n",
        "        self._check_if_trained()\n",
        "        # return self.use_cols\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        self._check_if_trained()\n",
        "        return self.model.feature_importances_\n",
        "\n",
        "\n",
        "def get_tree_model(name: str) -> Type[AbstractTreeModel]:\n",
        "    DEFINED_MODELS = {\n",
        "        \"lgb\": LGBModel,\n",
        "        \"xgb\": XGBModel,\n",
        "        \"cat\": CatBoostModel,\n",
        "    }\n",
        "    model = DEFINED_MODELS.get(name)\n",
        "    if model is None:\n",
        "        raise ValueError(\n",
        "            \"\"\"Invalid model name: {}.\n",
        "            Pre-defined model names are as follows: {}\"\"\".format(\n",
        "                name,\n",
        "                \",\".join(DEFINED_MODELS.keys())\n",
        "            ))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70dJPb20ISl_"
      },
      "outputs": [],
      "source": [
        "def viz_feature_importances(feature_importances: pd.DataFrame)-> None:\n",
        "    fi_gby = feature_importances.groupby(\"feature\").agg({\n",
        "        \"importance\": [\"mean\", \"std\"]\n",
        "    }).sort_values((\"importance\", \"mean\"), ascending=0).index\n",
        "    plt.figure(figsize=(30, 20))\n",
        "    \n",
        "    sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=fi_gby)\n",
        "\n",
        "    plt.title('LightGBM Features (avg over folds)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm7PQWKwWS7W"
      },
      "outputs": [],
      "source": [
        "num_cols = [c for c in train_df.columns if train_df[c].dtype != np.object]\n",
        "if \"id\" in num_cols:\n",
        "    num_cols.remove(\"id\")\n",
        "if \"kfold\" in num_cols:\n",
        "    num_cols.remove(\"kfold\")\n",
        "num_cols.remove(\"pm25_mid\")\n",
        "\n",
        "target_cols = \"pm25_mid\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df0, test_df0 = preprocess_all(train_df, test_df)\n",
        "\n",
        "train_df1 = preprocess(train_df0)\n",
        "test_df1 = preprocess(test_df0)"
      ],
      "metadata": {
        "id": "xByJwe_0t2Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_df1 = pd.concat([train_df1, test_df1])\n",
        "\n",
        "# after_before_cs = [\"year\", \"month\", \"day\", \"co_mid\", \"o3_mid\", \"so2_mid\", \"no2_mid\", \"temperature_mid\", \"humidity_mid\", \"pressure_mid\", \"ws_mid\", \"dew_mid\",\n",
        "#             \"scaled_no2_mid+scaled_so2_mid+scaled_co_mid\"]\n",
        "\n",
        "# today_df = pd.merge(city_year_month_day, \n",
        "#             all_df1[after_before_cs + [\"City\"]],\n",
        "#             on=[\"City\", \"year\", \"month\", \"day\"], how = \"left\")\n",
        "\n",
        "# daybefore_df= today_df.groupby([\"City\"]).shift()\n",
        "# dayafter_df = today_df.groupby([\"City\"]).shift(-1)\n",
        "\n",
        "# columns_dic = {}\n",
        "# daybefore_cs = []\n",
        "# for c in after_before_cs:\n",
        "#     columns_dic[c] = f\"1dbefore_{c}\"\n",
        "#     daybefore_cs.append(f\"1dbefore_{c}\")\n",
        "\n",
        "# daybefore_df = daybefore_df.rename(columns = columns_dic)\n",
        "\n",
        "# columns_dic = {}\n",
        "# dayafter_cs = []\n",
        "# for c in after_before_cs:\n",
        "#     columns_dic[c] = f\"1dafter_{c}\"\n",
        "#     dayafter_cs.append(f\"1dafter_{c}\")\n",
        "\n",
        "# dayafter_df = dayafter_df.rename(columns = columns_dic)\n",
        "\n",
        "# beforeafterday_df = pd.concat([today_df[[\"City\", \"year\", \"month\", \"day\"]], daybefore_df[daybefore_cs], dayafter_df[dayafter_cs]], axis =1)"
      ],
      "metadata": {
        "id": "j3Gu_WsAlHrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IDW_ps = [1, 1.2, 1.5, 2]\n",
        "days = [-2, -1, 0, 1, 2]"
      ],
      "metadata": {
        "id": "kYXbdWitvZY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [\"Country\", \"City\", \"year_month\", \"year_season\",  \"year_month_day\", \"month_day\", \"month_day_city\", \"month_day_country\", \"nearest_city\"] \n",
        "\n",
        "day_columns = [\"co_mid\", \"o3_mid\", \"so2_mid\", \"no2_mid\", \"temperature_mid\", \"humidity_mid\", \"pressure_mid\", \"ws_mid\", \"dew_mid\"] + \\\n",
        "[\"scaled_no2_mid+scaled_so2_mid+scaled_o3_mid+scaled_co_mid\", \"scaled_co_mid\"] + \\\n",
        "[\"nearest_pm25_mid\", ] + \\\n",
        "[f\"IDW_p{p}_pm25_mid\" for p in IDW_ps] \n",
        "\n",
        "IDW_columns = [\"pm25_mid\", \"no2_mid\", \"so2_mid\", \"co_mid\", \"o3_mid\"]\n",
        "\n",
        "num_cols = list(set(train_df1.columns) - set(cat_cols + [\"pm25_mid\", \"kfold\", \"id\", \"year\"])) + \\\n",
        " [\"nearest_pm25_mid\"] + \\\n",
        "[f\"dis{i}km_pm25_mid\" for i in [100, 300, 500, 1000]] +\\\n",
        "[f\"IDW_p{p}_pm25_mid\" for p in IDW_ps]\n",
        "\n",
        "for p in IDW_ps:\n",
        "    for IDW_c in IDW_columns:\n",
        "        if IDW_c == \"pm25_mid\": continue\n",
        "        num_cols += [f\"IDW_p{p}_{IDW_c}\"] \n",
        "        num_cols += [f\"ratio_p{p}_{IDW_c}\"]  \n",
        "        num_cols += [f\"correction{IDW_c}_IDW_p{p}_pm25_mid\"]\n",
        "\n",
        "for day in days:\n",
        "    for day_column in day_columns:\n",
        "        if day == 0:\n",
        "            continue\n",
        "        num_cols += [f\"{day}d_{day_column}\"]\n",
        "\n",
        "use_cols = num_cols + cat_cols"
      ],
      "metadata": {
        "id": "EYX6Hj-bxCq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(use_cols))"
      ],
      "metadata": {
        "id": "PxfpqqnT08xY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### for tuning"
      ],
      "metadata": {
        "id": "1pnk9E7WP-Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# models = []\n",
        "\n",
        "# oof_df = train_df_[[\"id\"]]\n",
        "# oof_df[\"oof\"] = -1\n",
        "\n",
        "# from lightgbm import early_stopping\n",
        "# from lightgbm import log_evaluation\n",
        "\n",
        "# # for i in range(NUM_FOLDS):\n",
        "# i = 0\n",
        "# train_data = train_df_[train_df_[\"kfold\"] != i].reset_index(drop = True)\n",
        "# valid_data = train_df_[train_df_[\"kfold\"] == i].reset_index(drop = True)\n",
        "# print(\"-\" * 5 + f\"FOLD {i + 1}/{NUM_FOLDS}\" + \"-\"*5)\n",
        "# print(f\"train_data nums: {len(train_data)}, valid_data nums: {len(valid_data)}\")\n",
        "\n",
        "# # fold内での最近傍を探す\n",
        "# train_data = train_df_[train_df_[\"kfold\"] != i].reset_index(drop = True)\n",
        "# print(len(train_data[\"City\"].unique()))\n",
        "\n",
        "# # 同じfoldの中で最近傍な都市があった場合はその年のtarget_encodingを実施\n",
        "# # nearest_city_fold_df = copy.deepcopy(nearest_city_df)\n",
        "# fold_cities = train_data[\"City\"].unique()\n",
        "\n",
        "# fold_nearest_cities = []\n",
        "# for (base_c, city1, city2, city3, city4, city5) in nearest_sort_cities:\n",
        "#     if city1 in fold_cities:\n",
        "#         nearest_city = city1\n",
        "#     elif city2 in fold_cities:\n",
        "#         nearest_city = city2\n",
        "#     elif city3 in fold_cities:\n",
        "#         nearest_city = city3\n",
        "#     elif city4 in fold_cities:\n",
        "#         nearest_city = city4\n",
        "#     elif city5 in fold_cities:\n",
        "#         nearest_city = city5\n",
        "#     fold_nearest_cities.append([base_c, nearest_city])\n",
        "# fold_nearest_cities_df = pd.DataFrame(fold_nearest_cities, columns = [\"City\", \"nearest_city\"])\n",
        "\n",
        "# # add city distance\n",
        "# fold_nearest_cities_df[\"distance_km\"] = -1\n",
        "# for j in range(fold_nearest_cities_df.shape[0]):\n",
        "#     city, nearest_city = fold_nearest_cities_df.loc[j, [\"City\", \"nearest_city\"]]\n",
        "#     dis = nearest_cities[city][nearest_city]\n",
        "#     fold_nearest_cities_df.loc[j, \"distance_km\"] = dis.km\n",
        "\n",
        "# target_en_fillna_ = target_en_fillna.rename(columns = {\"City\": \"nearest_city\", \"pm25_mid\": \"nearest_pm25_mid\", \n",
        "#                                                         \"1dafter_pm25_mid\":\"nearest_1dafter_pm25_mid\", \"1dbefore_pm25_mid\":\"nearest_1dbefore_pm25_mid\"})\n",
        "# fold_nearest_city_target_en = fold_nearest_cities_df.merge(target_en_fillna_, how = \"left\", on= \"nearest_city\")\n",
        "\n",
        "# train_data = train_data.merge(fold_nearest_city_target_en, how = \"left\", on = [\"City\", \"year\", \"month\", \"day\"])\n",
        "# valid_data = valid_data.merge(fold_nearest_city_target_en, how = \"left\", on = [\"City\", \"year\", \"month\", \"day\"])\n",
        "\n",
        "# # 距離に応じた近傍の都市のpm25_mid\n",
        "# for dis in [50, 100, 300, 500, 1000]:\n",
        "#     train_data[f\"dis{dis}km_pm25_mid\"] = np.nan\n",
        "#     valid_data[f\"dis{dis}km_pm25_mid\"] = np.nan\n",
        "\n",
        "#     train_data.loc[train_data[\"distance_km\"] <= dis, f\"dis{dis}km_pm25_mid\"] = train_data.loc[train_data[\"distance_km\"] <= dis, \"nearest_pm25_mid\"]\n",
        "#     valid_data.loc[valid_data[\"distance_km\"] <= dis, f\"dis{dis}km_pm25_mid\"] = valid_data.loc[valid_data[\"distance_km\"] <= dis, \"nearest_pm25_mid\"]\n",
        "\n",
        "# # test\n",
        "# if i == 0:\n",
        "#     test_nearest_cities = []\n",
        "#     for (base_c, city1, city2, city3, city4, city5) in nearest_sort_cities:\n",
        "#         nearest_city = city1\n",
        "#         test_nearest_cities.append([base_c, nearest_city])\n",
        "#     test_nearest_cities_df = pd.DataFrame(test_nearest_cities, columns = [\"City\", \"nearest_city\"])\n",
        "    \n",
        "#     for j in range(test_nearest_cities_df.shape[0]):\n",
        "#         city, nearest_city = test_nearest_cities_df.loc[j, [\"City\", \"nearest_city\"]]\n",
        "#         dis = nearest_cities[city][nearest_city]\n",
        "#         test_nearest_cities_df.loc[j, \"distance_km\"] = dis.km\n",
        "\n",
        "#     test_nearest_city_target_en = test_nearest_cities_df.merge(target_en_fillna_, how=\"left\", on = \"nearest_city\")\n",
        "#     test_df_ = test_df_.merge(test_nearest_city_target_en, how=\"left\", on=[\"City\", \"year\", \"month\", \"day\"])\n",
        "#     for dis in [50, 100, 300, 500, 1000]:\n",
        "#         test_df_[f\"dis{dis}km_pm25_mid\"] = np.nan\n",
        "#         test_df_.loc[test_df_[\"distance_km\"] <= dis, f\"dis{dis}km_pm25_mid\"] = test_df_.loc[test_df_[\"distance_km\"] <= dis, \"nearest_pm25_mid\"]\n",
        "#     test_df__ = copy.deepcopy(test_df_)\n",
        "\n",
        "# for c in cat_cols:\n",
        "#     le = LabelEncoder()\n",
        "#     le.fit(pd.concat([train_data, valid_data, test_df_])[c])\n",
        "#     train_data[c] = le.transform(train_data[c])\n",
        "#     valid_data[c] = le.transform(valid_data[c])\n",
        "#     if i == 0:\n",
        "#         test_df__[c] = le.transform(test_df_[c])"
      ],
      "metadata": {
        "id": "QrL-CjWi1NDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lgb_train = lgb.Dataset(train_data[use_cols], train_data[target_cols])\n",
        "# lgb_eval = lgb.Dataset(valid_data[use_cols], valid_data[target_cols])"
      ],
      "metadata": {
        "id": "HSjaDOx4rVVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lgb_train = lgb.Dataset(train_data[use_cols], train_data[target_cols])\n",
        "# lgb_eval = lgb.Dataset(valid_data[use_cols], valid_data[target_cols])\n",
        "\n",
        "# def objective(trial):\n",
        "#     params= {\n",
        "#             'boosting': trial.suggest_categorical(\"boosting\", [\"gbdt\", \"rf\"]),\n",
        "#             'objective': 'rmse',\n",
        "#             'metric': 'rmse',\n",
        "#             'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
        "#             \"max_depth\": trial.suggest_categorical(\"max_depth\", [8, 16, 32]),\n",
        "#             \"num_leaves\": trial.suggest_categorical(\"num_leaves\", [8, 16, 32, 64]),\n",
        "#             'subsample': 0.7,\n",
        "#             'subsample_freq': 1,\n",
        "#             \"min_data_in_leaf\": 30,\n",
        "#             \"device\": \"gpu\",  \n",
        "#             \"seed\":2022,\n",
        "#     }\n",
        "#     model = get_tree_model(\"lgb\")()\n",
        "#     gbm = lgb.train(params, lgb_train, valid_sets=lgb_eval, **train_params)\n",
        "#     preds = gbm.predict(valid_data[use_cols])\n",
        "\n",
        "#     accuracy = np.sqrt(mean_squared_error(valid_data[target_cols], preds))\n",
        "#     return accuracy"
      ],
      "metadata": {
        "id": "xdYk00aprPpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# study = optuna.create_study(direction = \"minimize\")\n",
        "# study.optimize(objective, n_trials = 50)"
      ],
      "metadata": {
        "id": "7tD7RkWXrPtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "4V1kSh-TQDHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### lightGBM"
      ],
      "metadata": {
        "id": "5u0Lc4Hrq_Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params= {\n",
        "#         'boosting': 'gbdt',\n",
        "#         'objective': 'rmse',\n",
        "#         'metric': 'rmse',\n",
        "#         'learning_rate': 0.00685,\n",
        "#         \"max_depth\": 8,\n",
        "#         \"num_leaves\": 64,\n",
        "#         'subsample': 0.7,\n",
        "#         'subsample_freq': 1,\n",
        "#         \"min_data_in_leaf\":30, \n",
        "#         \"device\": \"gpu\",  \n",
        "#         \"seed\":2022,\n",
        "# }\n",
        "\n",
        "# train_params = {\n",
        "#     \"num_boost_round\": 50000,\n",
        "#     \"callbacks\": [\n",
        "#                   lgb.early_stopping(stopping_rounds= 1000, verbose=True),\n",
        "#                   lgb.log_evaluation(period = 500)\n",
        "#     ]\n",
        "# }"
      ],
      "metadata": {
        "id": "WBhtD8USrEyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### XGboost"
      ],
      "metadata": {
        "id": "EDSNcL1HrFgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# params = {\n",
        "#         \"objective\":\"reg:linear\",\n",
        "#         \"n_estimators\":20000,\n",
        "#         'eta': 0.05064176893220393,\n",
        "#         'max_depth': 8, \n",
        "#         'learning_rate': 0.00460851283030278,\n",
        "#         \"subsample\":0.7,\n",
        "#         \"eval_metric\":\"rmse\", \n",
        "#         \"seed\":123,\n",
        "#         \"tree_method\":\"gpu_hist\"\n",
        "\n",
        "# }\n",
        "\n",
        "# train_params = {\n",
        "#     \"verbose\":1000, \n",
        "#     \"early_stopping_rounds\":500\n",
        "# }"
      ],
      "metadata": {
        "id": "G0mPf_Q3q3LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### catboost パラメータ"
      ],
      "metadata": {
        "id": "-fus81_qrNts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\n",
        "    \"task_type\": \"GPU\",\n",
        "    \"iterations\": 20000,\n",
        "    # \"n_estimators\": 1000,\n",
        "    # \"num_trees\":8,\n",
        "    # \"learning_rate\":0.01,\n",
        "    # \"depth\":8,\n",
        "    \"loss_function\": \"RMSE\",\n",
        "    \"early_stopping_rounds\":500,\n",
        "    \"random_seed\":2022,\n",
        "    \"use_best_model\":True,\n",
        "}\n",
        "\n",
        "# iterations, n_estimators, num_boost_round, num_trees\n",
        "\n",
        "train_params = {\n",
        "    \"verbose\": 500,\n",
        "}\n",
        "# train_params = {\n",
        "#     # \"verbose\":1000, \n",
        "#     # \"early_stopping_rounds\":500\n",
        "# }"
      ],
      "metadata": {
        "id": "Mham3PeMrSGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(target_en_fillna.query(f'City == \"{city1}\"')[\"pm25_mid\"].values)"
      ],
      "metadata": {
        "id": "8_faJtBHypuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "\n",
        "train_df_ = copy.deepcopy(train_df1)\n",
        "test_df_ = copy.deepcopy(test_df1)\n",
        "\n",
        "oof_df = train_df_[[\"id\"]]\n",
        "oof_df[\"oof\"] = -1\n",
        "\n",
        "from lightgbm import early_stopping\n",
        "from lightgbm import log_evaluation\n",
        "\n",
        "part_target_en_fillna = target_en_fillna[[\"City\", \"year\", \"month\", \"day\"] + IDW_columns]\n",
        "target_en_dic_bycity = {}\n",
        "for IDW_c in IDW_columns:\n",
        "    target_en_dic_bycity[IDW_c] = {}\n",
        "    for city in tqdm(part_target_en_fillna[\"City\"].unique()):\n",
        "        target_en_dic_bycity[IDW_c][city] = part_target_en_fillna.query(f\"City == '{city}'\")[f\"{IDW_c}\"].values \n",
        "\n",
        "for i in range(NUM_FOLDS):\n",
        "    model = get_tree_model(\"cat\")()\n",
        "    train_data = train_df_[train_df_[\"kfold\"] != i].reset_index(drop = True)\n",
        "    valid_data = train_df_[train_df_[\"kfold\"] == i].reset_index(drop = True)\n",
        "    print(\"-\" * 5 + f\"FOLD {i + 1}/{NUM_FOLDS}\" + \"-\"*5)\n",
        "    print(f\"train_data nums: {len(train_data)}, valid_data nums: {len(valid_data)}\")\n",
        "\n",
        "    # fold内での最近傍を探す\n",
        "    train_data = train_df_[train_df_[\"kfold\"] != i].reset_index(drop = True)\n",
        "    print(len(train_data[\"City\"].unique()))\n",
        "\n",
        "    # 同じfoldの中で最近傍な都市があった場合はその年のtarget_encodingを実施\n",
        "    # nearest_city_fold_df = copy.deepcopy(nearest_city_df)\n",
        "    fold_cities = train_data[\"City\"].unique()\n",
        "\n",
        "    fold_nearest_cities = []\n",
        "    for (base_c, city1, city2, city3, city4, city5) in nearest_sort_cities:\n",
        "        if city1 in fold_cities:\n",
        "            nearest_city = city1\n",
        "        elif city2 in fold_cities:\n",
        "            nearest_city = city2\n",
        "        elif city3 in fold_cities:\n",
        "            nearest_city = city3\n",
        "        elif city4 in fold_cities:\n",
        "            nearest_city = city4\n",
        "        elif city5 in fold_cities:\n",
        "            nearest_city = city5\n",
        "        fold_nearest_cities.append([base_c, nearest_city])\n",
        "    fold_nearest_cities_df = pd.DataFrame(fold_nearest_cities, columns = [\"City\", \"nearest_city\"])\n",
        "\n",
        "    # add city distance\n",
        "    fold_nearest_cities_df[\"distance_km\"] = -1\n",
        "    for j in range(fold_nearest_cities_df.shape[0]):\n",
        "        city, nearest_city = fold_nearest_cities_df.loc[j, [\"City\", \"nearest_city\"]]\n",
        "        dis = nearest_cities[city][nearest_city]\n",
        "        fold_nearest_cities_df.loc[j, \"distance_km\"] = dis.km\n",
        "\n",
        "    target_en_fillna_ = target_en_fillna.rename(columns = {\"City\": \"nearest_city\", \"pm25_mid\": \"nearest_pm25_mid\", \"co_mid\": \"co_mid2\", \n",
        "                                                           \"so2_mid\": \"so2_mid2\", \"no2_mid\": \"no2_mid2\", \"o3_mid\": \"o3_mid2\"})\n",
        "    fold_nearest_city_target_en = fold_nearest_cities_df.merge(target_en_fillna_, how = \"left\", on= \"nearest_city\")\n",
        "    train_data = train_data.merge(fold_nearest_city_target_en, how = \"left\", on = [\"City\", \"year\", \"month\", \"day\"])\n",
        "    valid_data = valid_data.merge(fold_nearest_city_target_en, how = \"left\", on = [\"City\", \"year\", \"month\", \"day\"])\n",
        "    \n",
        "# IDWによる計算(https://mf-atelier.sakura.ne.jp/mf-atelier2/a40/)\n",
        "    part_target_en_fillna = target_en_fillna[[\"City\", \"year\", \"month\", \"day\"] + IDW_columns]\n",
        "    for p in IDW_ps:\n",
        "        for IDW_c in IDW_columns:\n",
        "            part_target_en_fillna[f\"IDW_p{p}_{IDW_c}\"] = np.nan\n",
        "            for city in train_cities:\n",
        "                sum_w = 0\n",
        "                sum_w_u = 0\n",
        "                for other_city, dis in nearest_cities[city].items():\n",
        "                    if not other_city in fold_cities:\n",
        "                        continue\n",
        "                # fold_nearest_cities_dfに含まれている都市との距離とpm25の値が欲しい\n",
        "                    w = 1 / (dis.km ** p)\n",
        "                    u = target_en_dic_bycity[IDW_c][other_city]\n",
        "                    u_nan_mask = np.nan_to_num(u)\n",
        "                    u_nan_flg = np.isnan(u)\n",
        "                    sum_w_u += w * u_nan_mask\n",
        "                    sum_w += w * (1 - u_nan_flg)\n",
        "                part_target_en_fillna.loc[part_target_en_fillna[\"City\"] == city, f\"IDW_p{p}_{IDW_c}\"] = sum_w_u / sum_w\n",
        "    part_target_en_fillna = part_target_en_fillna.drop(columns = IDW_columns)\n",
        "    train_data = train_data.merge(part_target_en_fillna, how = \"left\", on = [\"City\", \"year\", \"month\", \"day\"])\n",
        "    valid_data = valid_data.merge(part_target_en_fillna, how = \"left\", on = [\"City\", \"year\", \"month\", \"day\"])\n",
        "    # 距離に応じた近傍の都市のpm25_mid    \n",
        "    for dis in [100, 300, 500, 1000]:\n",
        "        train_data[f\"dis{dis}km_pm25_mid\"] = np.nan\n",
        "        valid_data[f\"dis{dis}km_pm25_mid\"] = np.nan\n",
        "        train_data.loc[train_data[\"distance_km\"] <= dis, f\"dis{dis}km_pm25_mid\"] = train_data.loc[train_data[\"distance_km\"] <= dis, \"nearest_pm25_mid\"]\n",
        "        valid_data.loc[valid_data[\"distance_km\"] <= dis, f\"dis{dis}km_pm25_mid\"] = valid_data.loc[valid_data[\"distance_km\"] <= dis, \"nearest_pm25_mid\"]\n",
        "\n",
        "    # 日付をずらす\n",
        "    train_valid_data = pd.concat([train_data, valid_data])\n",
        "    days_shift_df = city_year_month_day.merge(train_valid_data[day_columns + [\"City\", \"year\", \"month\", \"day\"]], on = [\"City\", \"year\", \"month\", \"day\"], how = \"left\")\n",
        "    for day in days: \n",
        "        for day_column in day_columns:\n",
        "            if day == 0:\n",
        "                continue\n",
        "            days_shift_df[f\"{day}d_{day_column}\"] = days_shift_df.groupby(\"City\").shift(day)[day_column]\n",
        "\n",
        "    days_shift_df = days_shift_df.drop(columns = day_columns)\n",
        "\n",
        "    train_data = train_data.merge(days_shift_df, how = \"left\",  on = [\"City\", \"year\", \"month\", \"day\"])\n",
        "    valid_data = valid_data.merge(days_shift_df, how = \"left\",  on = [\"City\", \"year\", \"month\", \"day\"])\n",
        "\n",
        "    for p in IDW_ps:\n",
        "        for IDW_c in IDW_columns:\n",
        "            if IDW_c == \"pm25_mid\": continue\n",
        "            train_data[f\"ratio_p{p}_{IDW_c}\"] = train_data[f\"{IDW_c}\"] / (train_data[f\"IDW_p{p}_{IDW_c}\"] + 1e-3)\n",
        "            train_data[f\"correction{IDW_c}_IDW_p{p}_pm25_mid\"] = train_data[f\"ratio_p{p}_{IDW_c}\"] * train_data[f\"IDW_p{p}_pm25_mid\"]\n",
        "            valid_data[f\"ratio_p{p}_{IDW_c}\"] = valid_data[f\"{IDW_c}\"] / (valid_data[f\"IDW_p{p}_{IDW_c}\"] + 1e-3)\n",
        "            valid_data[f\"correction{IDW_c}_IDW_p{p}_pm25_mid\"] = valid_data[f\"ratio_p{p}_{IDW_c}\"] * valid_data[f\"IDW_p{p}_pm25_mid\"]\n",
        "\n",
        "    train_data = train_data.merge(oof_df_[[\"id\", \"pseude_pm25_mid\"]], how = \"left\",  on = [\"id\"])\n",
        "    valid_data = valid_data.merge(oof_df_[[\"id\", \"pseude_pm25_mid\"]], how = \"left\",  on = [\"id\"])\n",
        "\n",
        "    # test\n",
        "    if i == 0:\n",
        "        test_nearest_cities = []\n",
        "        for (base_c, city1, city2, city3, city4, city5) in nearest_sort_cities:\n",
        "            nearest_city = city1\n",
        "            test_nearest_cities.append([base_c, nearest_city])\n",
        "        test_nearest_cities_df = pd.DataFrame(test_nearest_cities, columns = [\"City\", \"nearest_city\"])\n",
        "        \n",
        "        for j in range(test_nearest_cities_df.shape[0]):\n",
        "            city, nearest_city = test_nearest_cities_df.loc[j, [\"City\", \"nearest_city\"]]\n",
        "            dis = nearest_cities[city][nearest_city]\n",
        "            test_nearest_cities_df.loc[j, \"distance_km\"] = dis.km\n",
        "\n",
        "        test_nearest_city_target_en = test_nearest_cities_df.merge(target_en_fillna_, how=\"left\", on = \"nearest_city\")\n",
        "        test_df_ = test_df_.merge(test_nearest_city_target_en, how=\"left\", on=[\"City\", \"year\", \"month\", \"day\"])\n",
        "\n",
        "        fortest_target_en_fillna = target_en_fillna[[\"City\", \"year\", \"month\", \"day\"] + IDW_columns]\n",
        "        for p in IDW_ps:\n",
        "            for IDW_c in IDW_columns:\n",
        "                fortest_target_en_fillna[f\"IDW_p{p}_{IDW_c}\"] = np.nan\n",
        "                for city in all_cities:\n",
        "                    sum_w = 0\n",
        "                    sum_w_u = 0\n",
        "                    for other_city, dis in nearest_cities[city].items():\n",
        "                        if not other_city in train_cities:\n",
        "                            continue\n",
        "                    # fold_nearest_cities_dfに含まれている都市との距離とpm25の値が欲しい\n",
        "                        w = 1 / (dis.km ** p)\n",
        "                        u = target_en_dic_bycity[IDW_c][other_city]\n",
        "                        u_nan_mask = np.nan_to_num(u)\n",
        "                        u_nan_flg = np.isnan(u)\n",
        "                        sum_w_u += w * u_nan_mask\n",
        "                        sum_w += w * (1 - u_nan_flg)\n",
        "                    fortest_target_en_fillna.loc[fortest_target_en_fillna[\"City\"] == city, f\"IDW_p{p}_{IDW_c}\"] = sum_w_u / sum_w\n",
        "        fortest_target_en_fillna = fortest_target_en_fillna.drop(columns = IDW_columns)\n",
        "        test_df_ = test_df_.merge(fortest_target_en_fillna, how = \"left\", on = [\"City\", \"year\", \"month\", \"day\"])    \n",
        "        for dis in [100, 300, 500, 1000]:\n",
        "            test_df_[f\"dis{dis}km_pm25_mid\"] = np.nan\n",
        "            test_df_.loc[test_df_[\"distance_km\"] <= dis, f\"dis{dis}km_pm25_mid\"] = test_df_.loc[test_df_[\"distance_km\"] <= dis, \"nearest_pm25_mid\"]\n",
        "\n",
        "        days_shift_df = city_year_month_day.merge(test_df_[day_columns + [\"City\", \"year\", \"month\", \"day\"]], on = [\"City\", \"year\", \"month\", \"day\"], how = \"left\")\n",
        "        for day in days:\n",
        "            for day_column in day_columns:\n",
        "                if day == 0:\n",
        "                    continue\n",
        "                days_shift_df[f\"{day}d_{day_column}\"] = days_shift_df.groupby(\"City\").shift(day)[day_column]\n",
        "        days_shift_df = days_shift_df.drop(columns = day_columns)\n",
        "        test_df_ = test_df_.merge(days_shift_df, how = \"left\",  on = [\"City\", \"year\", \"month\", \"day\"])\n",
        "        test_df_ = test_df_.merge(submit_df_[[\"id\", \"pseude_pm25_mid\"]], how = \"left\",  on = [\"id\"])\n",
        "        for p in IDW_ps:\n",
        "            for IDW_c in IDW_columns:\n",
        "                if IDW_c == \"pm25_mid\": continue\n",
        "                test_df_[f\"ratio_p{p}_{IDW_c}\"] = test_df_[f\"{IDW_c}\"] / (test_df_[f\"IDW_p{p}_{IDW_c}\"] + 1e-3)\n",
        "                test_df_[f\"correction{IDW_c}_IDW_p{p}_pm25_mid\"] = test_df_[f\"ratio_p{p}_{IDW_c}\"] * test_df_[f\"IDW_p{p}_pm25_mid\"]\n",
        "\n",
        "        test_df__ = copy.deepcopy(test_df_)\n",
        "\n",
        "    train_data__ = copy.deepcopy(train_data)\n",
        "    valid_data__ = copy.deepcopy(valid_data)\n",
        "    for c in cat_cols:\n",
        "        le = LabelEncoder()\n",
        "        le.fit(pd.concat([train_data, valid_data, test_df_])[c])\n",
        "        train_data__[c] = le.transform(train_data[c])\n",
        "        valid_data__[c] = le.transform(valid_data[c])\n",
        "        if i == 0:\n",
        "            test_df__[c] = le.transform(test_df_[c])\n",
        "            \n",
        "    model.train(\n",
        "        params,\n",
        "        train_params = train_params,\n",
        "        X_train = train_data__[use_cols],\n",
        "        y_train = train_data__[target_cols],\n",
        "        X_val = valid_data__[use_cols],\n",
        "        y_val = valid_data__[target_cols],\n",
        "    )\n",
        "    # save model\n",
        "    with open(os.path.join(SAVE_DIR, f\"model_{i}.pkl\"), \"wb\") as p:\n",
        "        pickle.dump(model, p)\n",
        "    models.append(model)\n",
        "\n",
        "    if i == 0:\n",
        "        feature_importance = pd.DataFrame()\n",
        "        feature_importance[\"feature\"] = use_cols\n",
        "        feature_importance[\"importance\"] = model.feature_importances_\n",
        "    else:\n",
        "        feature_importance[\"importance\"] += model.feature_importances_\n",
        "\n",
        "    oof = model.predict(\n",
        "        valid_data__[use_cols]\n",
        "    )\n",
        "    oof_df.loc[train_df[\"kfold\"] == i, \"oof\"] = oof\n",
        "\n",
        "train_oof = train_df.merge(oof_df[[\"id\", \"oof\"]], on = \"id\", how = \"left\")"
      ],
      "metadata": {
        "id": "JhpMtGvlq3Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in train_data__.columns:\n",
        "    print(c)"
      ],
      "metadata": {
        "id": "LvlT-dXazbtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz_feature_importances(feature_importance)"
      ],
      "metadata": {
        "id": "RJWd2VaA1NGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance.groupby(\"feature\").agg({\n",
        "    \"importance\": [\"mean\", \"std\"]\n",
        "}).sort_values((\"importance\", \"mean\"), ascending=0).head(50)"
      ],
      "metadata": {
        "id": "ky-KxgJZgvgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.clip(train_oof[\"oof\"], 0, 1000)\n",
        "oof_score = np.sqrt(mean_squared_error(train_oof[\"pm25_mid\"], a))\n",
        "oof_score = format(oof_score, \".4f\")\n",
        "print(f\"oof score: {oof_score}\")\n",
        "\n",
        "### 21.3638 normal\n",
        "### 21.4486  add population data\n",
        "### 21.4662 add co2 footprint\n",
        "### 21.3909 normal + clipped\n",
        "### 21.3909 normal IDW_ps += 3,5, days -3\n",
        "### 1000km以内のIDW"
      ],
      "metadata": {
        "id": "t16XNLJ21NIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(a, bins = 100, label = \"predict\")\n",
        "plt.hist(train_oof[\"pm25_mid\"], bins = 100, label = \"target\", alpha = 0.4)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rVo127W6eE22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAWRkjDaaIta"
      },
      "outputs": [],
      "source": [
        "train_oof[[\"id\",\"pm25_mid\", \"kfold\", \"oof\"]].to_csv(os.path.join(OOF_DIR, \"oof.csv\"), index= False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "for i in range(NUM_FOLDS):\n",
        "    with open(os.path.join(SAVE_DIR, f\"model_{i}.pkl\"), \"rb\") as p:\n",
        "        model = pickle.load(p)\n",
        "    models.append(model)"
      ],
      "metadata": {
        "id": "se56LnpAZaaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vAXJVnWSySV"
      },
      "source": [
        "# predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1434DZVHg0W"
      },
      "outputs": [],
      "source": [
        "sum_predict = None\n",
        "for model in models:\n",
        "    predict = model.predict(\n",
        "        test_df__[use_cols],\n",
        "    )\n",
        "    if sum_predict is None:\n",
        "        sum_predict = predict\n",
        "    else:\n",
        "        sum_predict += predict\n",
        "\n",
        "mean_predict = sum_predict / len(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjcrS25rrje9"
      },
      "outputs": [],
      "source": [
        "print(len(mean_predict))\n",
        "print(sample_df.shape)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xot64X_yHg4L"
      },
      "outputs": [],
      "source": [
        "sample_df.loc[:,\"predict\"] = np.clip(mean_predict, 0, 450)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVae9iW6cL0D"
      },
      "outputs": [],
      "source": [
        "sample_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87NkPMdTSrmH"
      },
      "outputs": [],
      "source": [
        "sample_df.to_csv(os.path.join(SUB_DIR, \"submission.csv\"), index= False, header = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(sample_df[\"predict\"], bins = 100)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kZeubahjojQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO0WqelkHyfS"
      },
      "source": [
        "# submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhTe6h6yDH8X"
      },
      "outputs": [],
      "source": [
        "!pip install signate > /dev/null\n",
        "!mkdir /root/.signate\n",
        "!cp /content/drive/MyDrive/signate/signate.json /root/.signate/signate.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSmQPSHLTX6E"
      },
      "outputs": [],
      "source": [
        "submission_file = os.path.join(SUB_DIR, \"submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yM6uPondJnm"
      },
      "outputs": [],
      "source": [
        "comment = f\"cv:{oof_score}_\" + NOTEBOOK_NAME\n",
        "comment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvoIkzVdibh-"
      },
      "source": [
        "# Submit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69-UNJ1YTuej"
      },
      "outputs": [],
      "source": [
        "!signate submit --competition-id=624 {submission_file} --note {comment}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OhKj-ijxqL1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdianTtHED1W"
      },
      "source": [
        "https://signate.jp/competitions/624/submissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrKqaQNXq6HS"
      },
      "outputs": [],
      "source": [
        "train_df[[\"City\", \"month\", \"pm25_mid\"]].groupby([\"City\"]).describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df__[[\"IDW_p1_pm25_mid\", \"IDW_p2_pm25_mid\"]]"
      ],
      "metadata": {
        "id": "Zf19grzU6L2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[[\"co_cnt\", \"co_var\", \"co_min\", \"co_max\", \"co_mid\"]]"
      ],
      "metadata": {
        "id": "qXndbaZZ5NF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen = np.random.default_rng()\n",
        "\n",
        "rnd = gen.normal(size=38)\n",
        "\n",
        "print(len(rnd), rnd.mean(), rnd.std())"
      ],
      "metadata": {
        "id": "TR28Gf_Q9J2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVDD0781EEep"
      },
      "outputs": [],
      "source": [
        "plt.hist(train_df[train_df[\"City\"] == \"Adapazarı\"][\"pm25_mid\"], bins = 100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TGGg9zDl4-nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "8vAXJVnWSySV",
        "ZO0WqelkHyfS"
      ],
      "machine_shape": "hm",
      "name": "FP077-076",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9123cf5a5bd447c1b03ef37de8112ade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cb2f7fa06a5b46c588c0c35df0858648",
              "IPY_MODEL_a066d6f2ffed4d009cf8256e28aeb03f",
              "IPY_MODEL_d6ad98f8376843e0b6af5e9783097d2f"
            ],
            "layout": "IPY_MODEL_13919d39bb244ca495f7ad91c508c787"
          }
        },
        "cb2f7fa06a5b46c588c0c35df0858648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0df84c0506524f24acdcaad04bdeaf34",
            "placeholder": "​",
            "style": "IPY_MODEL_fcd5c16e04ba4d66a411e00cfb57382e",
            "value": " 66%"
          }
        },
        "a066d6f2ffed4d009cf8256e28aeb03f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e1a774f7d804e8c9d321a1d3c89fd18",
            "max": 302,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68c0d3dea7b6487bb531f663c2c00d72",
            "value": 198
          }
        },
        "d6ad98f8376843e0b6af5e9783097d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_692d270a6dc74857b74056304a5c4079",
            "placeholder": "​",
            "style": "IPY_MODEL_59385a249a6646109f545c0e3df1c543",
            "value": " 198/302 [00:35&lt;00:15,  6.69it/s]"
          }
        },
        "13919d39bb244ca495f7ad91c508c787": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df84c0506524f24acdcaad04bdeaf34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcd5c16e04ba4d66a411e00cfb57382e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e1a774f7d804e8c9d321a1d3c89fd18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68c0d3dea7b6487bb531f663c2c00d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "692d270a6dc74857b74056304a5c4079": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59385a249a6646109f545c0e3df1c543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}