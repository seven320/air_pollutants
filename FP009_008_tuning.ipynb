{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FP009-008-tuning",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "8vAXJVnWSySV",
        "ZO0WqelkHyfS"
      ],
      "authorship_tag": "ABX9TyMA/n31Vz/e9wz04uLeuDfk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seven320/air_pollutants/blob/main/FP009_008_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Env"
      ],
      "metadata": {
        "id": "TCnFX004iEpo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OhibwrnFAQk",
        "outputId": "37853781-a4c3-47d6-f285-f89ef22020ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 19 12:03:08 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if COLAB:\n",
        "    from google.colab import drive\n",
        "    from google.colab import output\n",
        "    drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b09jV65gFc3Y",
        "outputId": "59410da4-1685-4787-dd14-978e79b26117"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if COLAB:\n",
        "    !pip install transformers > /dev/null\n",
        "    !pip install einops > /dev/null\n",
        "    !pip install optuna > /dev/null\n",
        "    # !pip install timm > /dev/null\n",
        "    # !pip install kaggle > /dev/null\n",
        "    # !pip install kaggle_datasets > /dev/null\n",
        "    # !pip install git+https://github.com/albumentations-team/albumentations\n",
        "    # !pip install tensorflow-determinism\n",
        "    !pip install -q iterative-stratification\n",
        "    !pip install python-Levenshtein > /dev/null\n",
        "\n",
        "    # for gpu\n",
        "    !git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "    %cd /content/LightGBM/\n",
        "    !mkdir build\n",
        "    !cmake -DUSE_GPU=1 \n",
        "    !make -j$(nproc)\n",
        "    !sudo apt-get -y install python-pip\n",
        "\n",
        "    !sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
        "    #pandasのエラーが出る場合は上記のコードからpandasを削除\n",
        "\n",
        "    !sudo -H pip install setuptools numpy scipy scikit-learn -U\n",
        "    %cd /content/LightGBM/python-package\n",
        "    !sudo python setup.py install --precompile\n",
        "\n",
        "    output.clear()"
      ],
      "metadata": {
        "id": "MvUvD3E2I6sj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import time\n",
        "import copy\n",
        "import warnings\n",
        "from requests import get\n",
        "from contextlib import contextmanager\n",
        "from typing import List, Optional, TypeVar, Type, Dict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GroupKFold, train_test_split, KFold\n",
        "from sklearn.preprocessing import RobustScaler, LabelEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "# from einops.layers.torch import Rearrange, Reduce\n",
        "# import timm\n",
        "import Levenshtein\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
        "from typing import Optional, Tuple\n",
        "from xgboost import XGBRegressor\n",
        "import lightgbm as lgb\n",
        "import optuna"
      ],
      "metadata": {
        "id": "VX7ZxFBMH3mX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B2Ml_n6_Cw7F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "3TLiFn8H_V90"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if COLAB:    \n",
        "    NOTEBOOK_NAME = get('http://172.28.0.2:9000/api/sessions').json()[0]['name']\n",
        "    print(NOTEBOOK_NAME)"
      ],
      "metadata": {
        "id": "jAG9b0Tb_YdF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dc291cc-a466-4171-9c93-eadafb7dec80"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP009-008-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "COMPE_NAME = \"air_pollutants\"\n",
        "BASE_DIR = f\"/content/drive/MyDrive/signate/{COMPE_NAME}\"\n",
        "\n",
        "# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu"
      ],
      "metadata": {
        "id": "BiWCgj_XI4y2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if COLAB:\n",
        "    INPUT_DIR = Path(os.path.join(BASE_DIR ,f\"input\"))\n",
        "    INPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    SAVE_DIR = Path(os.path.join(BASE_DIR ,f\"models/{NOTEBOOK_NAME}\"))\n",
        "    SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    OOF_DIR = Path(os.path.join(BASE_DIR, f\"oof/{NOTEBOOK_NAME}\"))\n",
        "    OOF_DIR.mkdir(exist_ok=True, parents = True)\n",
        "\n",
        "    SUB_DIR = Path(os.path.join(BASE_DIR, f\"submission/{NOTEBOOK_NAME}\"))\n",
        "    SUB_DIR.mkdir(exist_ok=True, parents = True)"
      ],
      "metadata": {
        "id": "tUAtLXzhI-Xr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(os.path.join(INPUT_DIR, \"train.csv\"))\n",
        "test_df = pd.read_csv(os.path.join(INPUT_DIR, \"test.csv\"))\n",
        "sample_df = pd.read_csv(os.path.join(INPUT_DIR, \"submit_sample.csv\"), names=[\"id\", \"predict\"])\n",
        "\n",
        "print(train_df.shape, test_df.shape, sample_df.shape)"
      ],
      "metadata": {
        "id": "z50ynnGpI-a-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de1a63be-6f78-446a-c51c-429ca0ae0410"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(195941, 54) (53509, 53) (53509, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# add city population"
      ],
      "metadata": {
        "id": "xJgrFC5tVZt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "population_df = pd.read_csv(os.path.join(INPUT_DIR, \"city_population.csv\"))\n",
        "population_df[\"City\"] = population_df[\"Name\"]"
      ],
      "metadata": {
        "id": "fD-VnMAxyd8F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 同じ都市名かつ国名のものを削除する\n",
        "population_df[\"CC\"] = population_df[\"Country\"] + population_df[\"City\"]\n",
        "population_df[\"CC\"].value_counts()\n",
        "for k, v in population_df[\"CC\"].value_counts().items():\n",
        "    # print(k, v)\n",
        "    if v > 1:\n",
        "        print(f\"{k} の行を削除\")\n",
        "        population_df = population_df[population_df[\"CC\"] != k]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyVhbHAGdwTx",
        "outputId": "78e8078d-7629-4adb-c29f-054fa192ff51"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChinaSuzhou の行を削除\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_country_dict(df: pd.DataFrame) -> Dict:\n",
        "    country_city = {}\n",
        "\n",
        "    for t_c in set(df[\"Country\"].tolist()):\n",
        "        country_city[t_c] = []\n",
        "\n",
        "    for country, city in zip(df[\"Country\"], df[\"City\"]):\n",
        "        country_city[country].append(city)\n",
        "\n",
        "    for k, v in country_city.items():\n",
        "        country_city[k] = set(v)\n",
        "\n",
        "    return country_city\n",
        "    "
      ],
      "metadata": {
        "id": "GHh1_7_-Dtka"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_country_city = df_to_country_dict(pd.concat([train_df, test_df]))\n",
        "\n",
        "p_country_city = df_to_country_dict(population_df)"
      ],
      "metadata": {
        "id": "JR3hx22DD7zE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cal_leven(str1: str, str2: str) -> float:\n",
        "    # レーベンシュタイン距離の取得\n",
        "    lev_dist = Levenshtein.distance(str1, str2)\n",
        "    # 標準化(長い方の文字列の長さで割る)\n",
        "    divider = len(str1) if len(str1) > len(str2) else len(str2)\n",
        "    lev_dist = lev_dist / divider\n",
        "    # 指標を合わせる(0:完全不一致 → 1:完全一致)\n",
        "    return 1 - lev_dist"
      ],
      "metadata": {
        "id": "1Y2yypTHJBdQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city_pairs = {}\n",
        "\n",
        "for t_k, t_values in train_country_city.items():\n",
        "    for p_k, p_values in p_country_city.items():\n",
        "        # countryが一致\n",
        "        if t_k != p_k:\n",
        "            continue\n",
        "        for t_v in t_values:\n",
        "            max_score = -1\n",
        "            for p_v in p_values:\n",
        "                score = cal_leven(t_v, p_v)\n",
        "                if max_score < score:\n",
        "                    pairs = [t_v, p_v]\n",
        "                    max_score = score\n",
        "\n",
        "            if max_score > 0.8:\n",
        "                city_pairs[pairs[1]] = pairs[0]\n",
        "print(f\"pair cities:{len(city_pairs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktKYCePw0-WV",
        "outputId": "dc8b5e85-6219-4336-87a0-22e30a7e53b2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pair cities:177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = population_df[\"City\"]\n",
        "for key, value in city_pairs.items():\n",
        "    # print(key, value)\n",
        "    a = copy.deepcopy(a.replace(key, value))\n",
        "\n",
        "population_df[\"City\"] = a"
      ],
      "metadata": {
        "id": "o7H-pU-SOcQR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "5znOfOv0YvQe",
        "outputId": "f10ed6e8-d373-4e21-9d1a-2ecadc574b3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            id  year  month  day        Country          City       lat  \\\n",
              "0            1  2019      1    1      Australia      Brisbane -27.46794   \n",
              "1            2  2019      1    1      Australia        Darwin -12.46113   \n",
              "2            3  2019      1    1      Australia     Melbourne -37.81400   \n",
              "3            4  2019      1    1      Australia     Newcastle -32.92953   \n",
              "4            5  2019      1    1      Australia         Perth -31.95224   \n",
              "...        ...   ...    ...  ...            ...           ...       ...   \n",
              "195936  195937  2021     12   31  United States  Jacksonville  30.33218   \n",
              "195937  195938  2021     12   31  United States     Las Vegas  36.17497   \n",
              "195938  195939  2021     12   31  United States     Milwaukee  43.03890   \n",
              "195939  195940  2021     12   31        Vietnam         Hanoi  21.02450   \n",
              "195940  195941  2021     12   31        Vietnam       Hạ Long  20.95045   \n",
              "\n",
              "              lon  co_cnt  co_min  ...  ws_min  ws_mid  ws_max  ws_var  \\\n",
              "0       153.02809      38   0.749  ...   0.241   1.088   3.101   1.983   \n",
              "1       130.84185      47   2.594  ...   0.828   3.473   7.396  10.411   \n",
              "2       144.96332      17   1.190  ...   0.000   2.107   8.089  15.719   \n",
              "3       151.78010      63   4.586  ...   0.284   0.503   3.592   2.485   \n",
              "4       115.86140      47   4.689  ...   0.500   0.755   3.396   1.937   \n",
              "...           ...     ...     ...  ...     ...     ...     ...     ...   \n",
              "195936  -81.65565      12   0.694  ...   2.195   2.710   6.125   3.757   \n",
              "195937 -115.13722      14   0.528  ...   1.002   2.974   6.861   8.354   \n",
              "195938  -87.90647     171   1.975  ...   0.994   1.087   2.578   0.612   \n",
              "195939  105.84117      31   2.613  ...   1.005   3.058   6.005   6.085   \n",
              "195940  107.07336      26   0.069  ...   0.190   2.775   3.412   2.528   \n",
              "\n",
              "        dew_cnt  dew_min  dew_mid  dew_max  dew_var  pm25_mid  \n",
              "0            17    7.671   10.358   15.112   13.424    19.901  \n",
              "1            62   21.324   23.813   24.221    2.021    13.741  \n",
              "2            22   10.309   13.133   15.422    6.355    25.918  \n",
              "3           116    7.146   10.685   13.344    9.417   174.370  \n",
              "4            93    1.091    3.277   12.272    4.109   167.063  \n",
              "...         ...      ...      ...      ...      ...       ...  \n",
              "195936       12   16.774   22.679   26.058   13.252    16.150  \n",
              "195937       12   10.432   14.741   15.827    7.078    16.895  \n",
              "195938       26    2.049    3.531    6.686    5.286    86.299  \n",
              "195939       51    1.922    7.443    7.716    4.642    36.523  \n",
              "195940       16    8.448   10.372   18.886   11.536    62.021  \n",
              "\n",
              "[195941 rows x 54 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f334bca-7d3f-426c-aefc-c6b151124d62\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>co_cnt</th>\n",
              "      <th>co_min</th>\n",
              "      <th>...</th>\n",
              "      <th>ws_min</th>\n",
              "      <th>ws_mid</th>\n",
              "      <th>ws_max</th>\n",
              "      <th>ws_var</th>\n",
              "      <th>dew_cnt</th>\n",
              "      <th>dew_min</th>\n",
              "      <th>dew_mid</th>\n",
              "      <th>dew_max</th>\n",
              "      <th>dew_var</th>\n",
              "      <th>pm25_mid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Brisbane</td>\n",
              "      <td>-27.46794</td>\n",
              "      <td>153.02809</td>\n",
              "      <td>38</td>\n",
              "      <td>0.749</td>\n",
              "      <td>...</td>\n",
              "      <td>0.241</td>\n",
              "      <td>1.088</td>\n",
              "      <td>3.101</td>\n",
              "      <td>1.983</td>\n",
              "      <td>17</td>\n",
              "      <td>7.671</td>\n",
              "      <td>10.358</td>\n",
              "      <td>15.112</td>\n",
              "      <td>13.424</td>\n",
              "      <td>19.901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Darwin</td>\n",
              "      <td>-12.46113</td>\n",
              "      <td>130.84185</td>\n",
              "      <td>47</td>\n",
              "      <td>2.594</td>\n",
              "      <td>...</td>\n",
              "      <td>0.828</td>\n",
              "      <td>3.473</td>\n",
              "      <td>7.396</td>\n",
              "      <td>10.411</td>\n",
              "      <td>62</td>\n",
              "      <td>21.324</td>\n",
              "      <td>23.813</td>\n",
              "      <td>24.221</td>\n",
              "      <td>2.021</td>\n",
              "      <td>13.741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Melbourne</td>\n",
              "      <td>-37.81400</td>\n",
              "      <td>144.96332</td>\n",
              "      <td>17</td>\n",
              "      <td>1.190</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000</td>\n",
              "      <td>2.107</td>\n",
              "      <td>8.089</td>\n",
              "      <td>15.719</td>\n",
              "      <td>22</td>\n",
              "      <td>10.309</td>\n",
              "      <td>13.133</td>\n",
              "      <td>15.422</td>\n",
              "      <td>6.355</td>\n",
              "      <td>25.918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Newcastle</td>\n",
              "      <td>-32.92953</td>\n",
              "      <td>151.78010</td>\n",
              "      <td>63</td>\n",
              "      <td>4.586</td>\n",
              "      <td>...</td>\n",
              "      <td>0.284</td>\n",
              "      <td>0.503</td>\n",
              "      <td>3.592</td>\n",
              "      <td>2.485</td>\n",
              "      <td>116</td>\n",
              "      <td>7.146</td>\n",
              "      <td>10.685</td>\n",
              "      <td>13.344</td>\n",
              "      <td>9.417</td>\n",
              "      <td>174.370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Perth</td>\n",
              "      <td>-31.95224</td>\n",
              "      <td>115.86140</td>\n",
              "      <td>47</td>\n",
              "      <td>4.689</td>\n",
              "      <td>...</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.755</td>\n",
              "      <td>3.396</td>\n",
              "      <td>1.937</td>\n",
              "      <td>93</td>\n",
              "      <td>1.091</td>\n",
              "      <td>3.277</td>\n",
              "      <td>12.272</td>\n",
              "      <td>4.109</td>\n",
              "      <td>167.063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195936</th>\n",
              "      <td>195937</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>United States</td>\n",
              "      <td>Jacksonville</td>\n",
              "      <td>30.33218</td>\n",
              "      <td>-81.65565</td>\n",
              "      <td>12</td>\n",
              "      <td>0.694</td>\n",
              "      <td>...</td>\n",
              "      <td>2.195</td>\n",
              "      <td>2.710</td>\n",
              "      <td>6.125</td>\n",
              "      <td>3.757</td>\n",
              "      <td>12</td>\n",
              "      <td>16.774</td>\n",
              "      <td>22.679</td>\n",
              "      <td>26.058</td>\n",
              "      <td>13.252</td>\n",
              "      <td>16.150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195937</th>\n",
              "      <td>195938</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>United States</td>\n",
              "      <td>Las Vegas</td>\n",
              "      <td>36.17497</td>\n",
              "      <td>-115.13722</td>\n",
              "      <td>14</td>\n",
              "      <td>0.528</td>\n",
              "      <td>...</td>\n",
              "      <td>1.002</td>\n",
              "      <td>2.974</td>\n",
              "      <td>6.861</td>\n",
              "      <td>8.354</td>\n",
              "      <td>12</td>\n",
              "      <td>10.432</td>\n",
              "      <td>14.741</td>\n",
              "      <td>15.827</td>\n",
              "      <td>7.078</td>\n",
              "      <td>16.895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195938</th>\n",
              "      <td>195939</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>United States</td>\n",
              "      <td>Milwaukee</td>\n",
              "      <td>43.03890</td>\n",
              "      <td>-87.90647</td>\n",
              "      <td>171</td>\n",
              "      <td>1.975</td>\n",
              "      <td>...</td>\n",
              "      <td>0.994</td>\n",
              "      <td>1.087</td>\n",
              "      <td>2.578</td>\n",
              "      <td>0.612</td>\n",
              "      <td>26</td>\n",
              "      <td>2.049</td>\n",
              "      <td>3.531</td>\n",
              "      <td>6.686</td>\n",
              "      <td>5.286</td>\n",
              "      <td>86.299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195939</th>\n",
              "      <td>195940</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>Hanoi</td>\n",
              "      <td>21.02450</td>\n",
              "      <td>105.84117</td>\n",
              "      <td>31</td>\n",
              "      <td>2.613</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005</td>\n",
              "      <td>3.058</td>\n",
              "      <td>6.005</td>\n",
              "      <td>6.085</td>\n",
              "      <td>51</td>\n",
              "      <td>1.922</td>\n",
              "      <td>7.443</td>\n",
              "      <td>7.716</td>\n",
              "      <td>4.642</td>\n",
              "      <td>36.523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195940</th>\n",
              "      <td>195941</td>\n",
              "      <td>2021</td>\n",
              "      <td>12</td>\n",
              "      <td>31</td>\n",
              "      <td>Vietnam</td>\n",
              "      <td>Hạ Long</td>\n",
              "      <td>20.95045</td>\n",
              "      <td>107.07336</td>\n",
              "      <td>26</td>\n",
              "      <td>0.069</td>\n",
              "      <td>...</td>\n",
              "      <td>0.190</td>\n",
              "      <td>2.775</td>\n",
              "      <td>3.412</td>\n",
              "      <td>2.528</td>\n",
              "      <td>16</td>\n",
              "      <td>8.448</td>\n",
              "      <td>10.372</td>\n",
              "      <td>18.886</td>\n",
              "      <td>11.536</td>\n",
              "      <td>62.021</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>195941 rows × 54 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f334bca-7d3f-426c-aefc-c6b151124d62')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f334bca-7d3f-426c-aefc-c6b151124d62 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f334bca-7d3f-426c-aefc-c6b151124d62');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not \"Prev\" in train_df.columns:\n",
        "    train_df = train_df.merge(population_df[[\"rank\", \"City\", \"Country\", \"Population\", \"Prev\", \"Growth\"]], on = [\"City\", \"Country\"], how=\"left\")\n",
        "    test_df = test_df.merge(population_df[[\"rank\", \"City\", \"Country\",  \"Population\", \"Prev\", \"Growth\"]], on = [\"City\", \"Country\"], how=\"left\")\n",
        "\n",
        "assert len(test_df) == len(sample_df)"
      ],
      "metadata": {
        "id": "o5b40dm-zrPh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "NUM_FOLDS = 5"
      ],
      "metadata": {
        "id": "pab-ry5T83Xa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############\n",
        "## Utility ##\n",
        "#############\n",
        "@contextmanager\n",
        "def timer(name: str):\n",
        "    t0 = time.time()\n",
        "    print(f\"[{name}] start\")\n",
        "    yield\n",
        "    print(f\"[{name}] done - elapsed {time.time() - t0:.2f}s\")"
      ],
      "metadata": {
        "id": "QO13XyUmI-dG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    \"\"\"\n",
        "    Seeds basic parameters for reproductibility of results\n",
        "    \n",
        "    Arguments:\n",
        "        seed {int} -- Number of the seed\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "IeHnNhCrJNvb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "id": "1JcbNGglJNxv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "8e595bf5-cb39-40f7-a296-ed00ccb06704"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id  year  month  day    Country       City       lat        lon  co_cnt  \\\n",
              "0   1  2019      1    1  Australia   Brisbane -27.46794  153.02809      38   \n",
              "1   2  2019      1    1  Australia     Darwin -12.46113  130.84185      47   \n",
              "2   3  2019      1    1  Australia  Melbourne -37.81400  144.96332      17   \n",
              "3   4  2019      1    1  Australia  Newcastle -32.92953  151.78010      63   \n",
              "4   5  2019      1    1  Australia      Perth -31.95224  115.86140      47   \n",
              "\n",
              "   co_min  ...  dew_cnt  dew_min  dew_mid  dew_max  dew_var  pm25_mid   rank  \\\n",
              "0   0.749  ...       17    7.671   10.358   15.112   13.424    19.901  201.0   \n",
              "1   2.594  ...       62   21.324   23.813   24.221    2.021    13.741    NaN   \n",
              "2   1.190  ...       22   10.309   13.133   15.422    6.355    25.918   79.0   \n",
              "3   4.586  ...      116    7.146   10.685   13.344    9.417   174.370    NaN   \n",
              "4   4.689  ...       93    1.091    3.277   12.272    4.109   167.063  242.0   \n",
              "\n",
              "   Population       Prev  Growth  \n",
              "0   2472222.0  2439467.0  0.0134  \n",
              "1         NaN        NaN     NaN  \n",
              "2   5150766.0  5061439.0  0.0176  \n",
              "3         NaN        NaN     NaN  \n",
              "4   2092649.0  2067333.0  0.0122  \n",
              "\n",
              "[5 rows x 58 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2ebe0792-e1a7-4ce7-a96a-7b15f8de1241\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>co_cnt</th>\n",
              "      <th>co_min</th>\n",
              "      <th>...</th>\n",
              "      <th>dew_cnt</th>\n",
              "      <th>dew_min</th>\n",
              "      <th>dew_mid</th>\n",
              "      <th>dew_max</th>\n",
              "      <th>dew_var</th>\n",
              "      <th>pm25_mid</th>\n",
              "      <th>rank</th>\n",
              "      <th>Population</th>\n",
              "      <th>Prev</th>\n",
              "      <th>Growth</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Brisbane</td>\n",
              "      <td>-27.46794</td>\n",
              "      <td>153.02809</td>\n",
              "      <td>38</td>\n",
              "      <td>0.749</td>\n",
              "      <td>...</td>\n",
              "      <td>17</td>\n",
              "      <td>7.671</td>\n",
              "      <td>10.358</td>\n",
              "      <td>15.112</td>\n",
              "      <td>13.424</td>\n",
              "      <td>19.901</td>\n",
              "      <td>201.0</td>\n",
              "      <td>2472222.0</td>\n",
              "      <td>2439467.0</td>\n",
              "      <td>0.0134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Darwin</td>\n",
              "      <td>-12.46113</td>\n",
              "      <td>130.84185</td>\n",
              "      <td>47</td>\n",
              "      <td>2.594</td>\n",
              "      <td>...</td>\n",
              "      <td>62</td>\n",
              "      <td>21.324</td>\n",
              "      <td>23.813</td>\n",
              "      <td>24.221</td>\n",
              "      <td>2.021</td>\n",
              "      <td>13.741</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Melbourne</td>\n",
              "      <td>-37.81400</td>\n",
              "      <td>144.96332</td>\n",
              "      <td>17</td>\n",
              "      <td>1.190</td>\n",
              "      <td>...</td>\n",
              "      <td>22</td>\n",
              "      <td>10.309</td>\n",
              "      <td>13.133</td>\n",
              "      <td>15.422</td>\n",
              "      <td>6.355</td>\n",
              "      <td>25.918</td>\n",
              "      <td>79.0</td>\n",
              "      <td>5150766.0</td>\n",
              "      <td>5061439.0</td>\n",
              "      <td>0.0176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Newcastle</td>\n",
              "      <td>-32.92953</td>\n",
              "      <td>151.78010</td>\n",
              "      <td>63</td>\n",
              "      <td>4.586</td>\n",
              "      <td>...</td>\n",
              "      <td>116</td>\n",
              "      <td>7.146</td>\n",
              "      <td>10.685</td>\n",
              "      <td>13.344</td>\n",
              "      <td>9.417</td>\n",
              "      <td>174.370</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Australia</td>\n",
              "      <td>Perth</td>\n",
              "      <td>-31.95224</td>\n",
              "      <td>115.86140</td>\n",
              "      <td>47</td>\n",
              "      <td>4.689</td>\n",
              "      <td>...</td>\n",
              "      <td>93</td>\n",
              "      <td>1.091</td>\n",
              "      <td>3.277</td>\n",
              "      <td>12.272</td>\n",
              "      <td>4.109</td>\n",
              "      <td>167.063</td>\n",
              "      <td>242.0</td>\n",
              "      <td>2092649.0</td>\n",
              "      <td>2067333.0</td>\n",
              "      <td>0.0122</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 58 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ebe0792-e1a7-4ce7-a96a-7b15f8de1241')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2ebe0792-e1a7-4ce7-a96a-7b15f8de1241 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2ebe0792-e1a7-4ce7-a96a-7b15f8de1241');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_city = train_df[\"City\"].unique().tolist()\n",
        "test_city = test_df[\"City\"].unique().tolist()\n",
        "\n",
        "train_city_cnt = len(train_city)\n",
        "test_city_cnt = len(test_city)\n",
        "\n",
        "print(f\"train city count: {train_city_cnt}\")\n",
        "print(f\"test city count: {test_city_cnt}\")\n",
        "\n",
        "print(f\"{len(set(train_city) & set(test_city))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZZhyY_iXJyQ",
        "outputId": "b0d5f159-1376-48bc-c511-77543bd7f519"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train city count: 239\n",
            "test city count: 63\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kf = GroupKFold(n_splits = NUM_FOLDS)\n",
        "\n",
        "if not \"kfold\" in train_df.columns:\n",
        "    folds = copy.deepcopy(train_df[[\"id\"]])\n",
        "    folds[\"kfold\"] = -1\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(kf.split(train_df, train_df[\"pm25_mid\"], train_df[\"City\"])):\n",
        "        print(f\"train_idx: {len(train_idx)}, valid_idx: {len(valid_idx)}\")\n",
        "        folds.loc[valid_idx, \"kfold\"] = fold\n",
        "\n",
        "    train_df = train_df.merge(folds[[\"id\", \"kfold\"]], on=\"id\", how=\"left\")"
      ],
      "metadata": {
        "id": "NAxJgCMWvzyq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d63a83a-7472-4370-8fc4-b60384fa78d7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_idx: 156755, valid_idx: 39186\n",
            "train_idx: 156758, valid_idx: 39183\n",
            "train_idx: 156760, valid_idx: 39181\n",
            "train_idx: 156730, valid_idx: 39211\n",
            "train_idx: 156761, valid_idx: 39180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "GgLI7LLc1szm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from typing import List, Optional, TypeVar\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class AbstractFeatureTransformer:\n",
        "    def __init__(self):\n",
        "        self.name = self.__class__.__name__\n",
        "\n",
        "    def fit_transform(self, input_df: pd.DataFrame, y=None):\n",
        "        self.fit(input_df, y)\n",
        "        return self.transform(input_df)\n",
        "\n",
        "    def fit(self, input_df: pd.DataFrame, y=None):\n",
        "        pass\n",
        "\n",
        "    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        raise NotImplementedError\n",
        "\n",
        "Transformer = TypeVar(\"Transformer\", bound=AbstractFeatureTransformer)\n",
        "\n",
        "def extract_features(input_df: pd.DataFrame,\n",
        "                     transformers: List[Transformer],\n",
        "                     fit: bool = True,\n",
        "                     logger: Optional[logging.Logger] = None):\n",
        "    feature_dfs = []\n",
        "    for transformer in transformers:\n",
        "        # timerはブロックの実行時間を計測するユーティリティ\n",
        "        with timer(f\"Extract features with {transformer.name}\", logger):\n",
        "            if fit:\n",
        "                feature_dfs.append(transformer.fit_transform(input_df))\n",
        "            else:\n",
        "                feature_dfs.append(transformer.transform(input_df))\n",
        "    all_features = pd.concat(feature_dfs, axis=1)\n",
        "    return all_features"
      ],
      "metadata": {
        "id": "ckDad6r_AevZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Numericals(AbstractFeatureTransformer):\n",
        "    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        cols = [pd.api.types.is_numeric_dtype(dtype) for dtype in input_df.dtypes]\n",
        "        return input_df.loc[:, cols]\n",
        "\n",
        "class LabelEncoding(AbstractFeatureTransformer):\n",
        "    def __init__(self, columns: List[str]):\n",
        "        super().__init__()\n",
        "        self.le_columns = columns\n",
        "        self.encoders = {\n",
        "            column: LabelEncoder()\n",
        "            for column in self.le_columns\n",
        "        }\n",
        "        self.__is_fitted = False\n",
        "\n",
        "    def fit(self, input_df: pd.DataFrame, y: Optional[np.ndarray] = None):\n",
        "        \"\"\"\n",
        "        ラベルに変換する前に欠損値を埋める\n",
        "        \"\"\"\n",
        "        for column in self.le_columns:\n",
        "            self.encoders[column].fit(input_df[column].fillna(\"\"))\n",
        "        self.__is_fitted = True\n",
        "\n",
        "    def transform(\n",
        "        self, \n",
        "        input_df: pd.DataFrame, \n",
        "        y: Optional[np.ndarray] = None\n",
        "    ) -> pd.DataFrame:\n",
        "        assert self.__is_fitted, \"You need to call `fit` first.\"\n",
        "        encoded = {}\n",
        "        for column in self.le_columns:\n",
        "            encoded[column] = self.encoders[column].transform(\n",
        "                input_df[column].fillna(\"\"))\n",
        "        return pd.DataFrame(encoded)"
      ],
      "metadata": {
        "id": "HrG0N8hZDsoR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df:pd.DataFrame)->pd.DataFrame:\n",
        "    df_ = copy.deepcopy(df)\n",
        "    # 経度をcosに変換\n",
        "    cnt = 0\n",
        "\n",
        "    df_[\"lon_cos\"] = np.cos(np.radians((df_[\"lon\"] + 180)))\n",
        "    df_[\"lon_sin\"] = np.sin(np.radians((df_[\"lon\"] + 180)))\n",
        "    df_[\"month_cos\"] = np.cos(np.radians(df_[\"month\"] / 12 * 360))\n",
        "    df_[\"month_sin\"] = np.sin(np.radians(df_[\"month\"] / 12 * 360))\n",
        "    cnt += 4\n",
        "\n",
        "    # 四則演算\n",
        "\n",
        "    # 南半球を北半球のmonthに変更\n",
        "    df_[\"month_world\"] = df_[\"month\"]\n",
        "    df_.loc[df_[\"lat\"] < 0, \"month_world\"] = df_[\"month\"] + 6\n",
        "    df_.loc[df_[\"month_world\"] > 12, \"month_world\"] = df_[\"month\"] - 6\n",
        "\n",
        "    cnt += 1\n",
        "    # mid同士を4則\n",
        "    columns = df_.columns\n",
        "    mids = []\n",
        "    for c in columns:\n",
        "        if \"_mid\" in c and c != \"pm25_mid\":\n",
        "            mids.append(c)\n",
        "    scaler = MinMaxScaler()\n",
        "    \n",
        "    # scaling_columns = []\n",
        "    # for i in range(len(mids)):\n",
        "    #     for j in range(i):\n",
        "    #         df_[f\"{mids[i]}x{mids[j]}\"] = df_[mids[i]] * df_[mids[j]]\n",
        "    #         df_[f\"{mids[i]}/{mids[j]}\"] = df_[mids[i]] / (df_[mids[j]] + 1e-6)\n",
        "    #         df_[f\"{mids[i]}+{mids[j]}\"] = df_[mids[i]] + df_[mids[j]]\n",
        "    #         df_[f\"{mids[i]}-{mids[j]}\"] = df_[mids[i]] - df_[mids[j]]\n",
        "    #         cnt += 4\n",
        "\n",
        "    #         scaling_columns += [f\"{mids[i]}x{mids[j]}\", f\"{mids[i]}/{mids[j]}\", f\"{mids[i]}+{mids[j]}\", f\"{mids[i]}-{mids[j]}\"]\n",
        "    print(f\"add {cnt} columns\")\n",
        "\n",
        "    # df_[scaling_columns] = scaler.fit_transform(df_[scaling_columns])\n",
        "\n",
        "    print(\"scaled columns\")\n",
        "\n",
        "    return df_"
      ],
      "metadata": {
        "id": "QNf23jZJLku0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label encoding\n",
        "\n",
        "cat_cols = [\"Country\", \"City\"]\n",
        "\n",
        "for c in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    le.fit(pd.concat([train_df, test_df])[c])\n",
        "    train_df[c] = le.transform(train_df[c])\n",
        "    test_df[c] = le.transform(test_df[c])"
      ],
      "metadata": {
        "id": "BzCu4V4EnvT6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_ = preprocess(train_df)\n",
        "test_df_ = preprocess(test_df)"
      ],
      "metadata": {
        "id": "m7mclN_sNFiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d042906-ebcb-4541-c67e-466dde7eaa2f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "add 5 columns\n",
            "scaled columns\n",
            "add 5 columns\n",
            "scaled columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_.head(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "4QfrMHWLNLcC",
        "outputId": "1fedb2f4-45e7-441e-bbaf-a722af9265e5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    id  year  month  day  Country  City       lat        lon  co_cnt  co_min  \\\n",
              "0    1  2019      1    1        0    24 -27.46794  153.02809      38   0.749   \n",
              "1    2  2019      1    1        0    54 -12.46113  130.84185      47   2.594   \n",
              "2    3  2019      1    1        0   146 -37.81400  144.96332      17   1.190   \n",
              "3    4  2019      1    1        0   176 -32.92953  151.78010      63   4.586   \n",
              "4    5  2019      1    1        0   193 -31.95224  115.86140      47   4.689   \n",
              "5    6  2019      1    1        0   278 -34.42400  150.89345     179   4.554   \n",
              "6    7  2019      1    1        1     8  51.22047    4.40026      93   4.011   \n",
              "7    8  2019      1    1        1    38  50.41136    4.44448      11   0.096   \n",
              "8    9  2019      1    1        1   137  50.63373    5.56749      12   0.097   \n",
              "9   10  2019      1    1        3   247 -23.54750  -46.63611     354   1.048   \n",
              "10  11  2019      1    1        3   275 -20.31944  -40.33778      29   0.117   \n",
              "11  12  2019      1    1        4   272  49.24966 -123.11934      34   0.110   \n",
              "12  13  2019      1    1        5    31 -22.45667  -68.92371      22   2.320   \n",
              "13  14  2019      1    1        5   208 -33.04752  -71.44249      28   0.078   \n",
              "14  15  2019      1    1        5   255 -35.42640  -71.65542     241   0.058   \n",
              "15  16  2019      1    1        6    15  39.90750  116.39723     199   5.209   \n",
              "16  17  2019      1    1        6    35  28.19874  112.97087      47   0.073   \n",
              "17  18  2019      1    1        6    40  30.66667  104.06667     336   5.015   \n",
              "18  19  2019      1    1        6    65  23.02677  113.13148     162   2.152   \n",
              "19  20  2019      1    1        6    68  41.88669  123.94363     103   2.543   \n",
              "\n",
              "    ...   rank  Population        Prev  Growth  kfold   lon_cos   lon_sin  \\\n",
              "0   ...  201.0   2472222.0   2439467.0  0.0134      0  0.891229 -0.453554   \n",
              "1   ...    NaN         NaN         NaN     NaN      2  0.653973 -0.756518   \n",
              "2   ...   79.0   5150766.0   5061439.0  0.0176      2  0.818785 -0.574101   \n",
              "3   ...    NaN         NaN         NaN     NaN      1  0.881139 -0.472857   \n",
              "4   ...  242.0   2092649.0   2067333.0  0.0122      1  0.436196 -0.899852   \n",
              "5   ...    NaN         NaN         NaN     NaN      0  0.873717 -0.486435   \n",
              "6   ...    NaN         NaN         NaN     NaN      3 -0.997052 -0.076724   \n",
              "7   ...    NaN         NaN         NaN     NaN      3 -0.996993 -0.077493   \n",
              "8   ...    NaN         NaN         NaN     NaN      0 -0.995283 -0.097018   \n",
              "9   ...    5.0  22429800.0  22237472.0  0.0086      0 -0.686629  0.727008   \n",
              "10  ...    NaN         NaN         NaN     NaN      0 -0.762242  0.647293   \n",
              "11  ...  191.0   2631690.0   2606351.0  0.0097      3  0.546385  0.837534   \n",
              "12  ...    NaN         NaN         NaN     NaN      0 -0.359611  0.933102   \n",
              "13  ...    NaN         NaN         NaN     NaN      2 -0.318256  0.948005   \n",
              "14  ...    NaN         NaN         NaN     NaN      3 -0.314731  0.949181   \n",
              "15  ...    8.0  21333332.0  20896820.0  0.0209      4  0.444592 -0.895733   \n",
              "16  ...   84.0   4809887.0   4694722.0  0.0245      2  0.390263 -0.920703   \n",
              "17  ...   36.0   9478521.0   9305116.0  0.0186      3  0.243051 -0.970014   \n",
              "18  ...   54.0   7497263.0   7406751.0  0.0122      1  0.392842 -0.919606   \n",
              "19  ...    NaN         NaN         NaN     NaN      1  0.558377 -0.829587   \n",
              "\n",
              "    month_cos  month_sin  month_world  \n",
              "0    0.866025        0.5            7  \n",
              "1    0.866025        0.5            7  \n",
              "2    0.866025        0.5            7  \n",
              "3    0.866025        0.5            7  \n",
              "4    0.866025        0.5            7  \n",
              "5    0.866025        0.5            7  \n",
              "6    0.866025        0.5            1  \n",
              "7    0.866025        0.5            1  \n",
              "8    0.866025        0.5            1  \n",
              "9    0.866025        0.5            7  \n",
              "10   0.866025        0.5            7  \n",
              "11   0.866025        0.5            1  \n",
              "12   0.866025        0.5            7  \n",
              "13   0.866025        0.5            7  \n",
              "14   0.866025        0.5            7  \n",
              "15   0.866025        0.5            1  \n",
              "16   0.866025        0.5            1  \n",
              "17   0.866025        0.5            1  \n",
              "18   0.866025        0.5            1  \n",
              "19   0.866025        0.5            1  \n",
              "\n",
              "[20 rows x 64 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8face887-a3dd-4fbb-8dac-af6ee7c48755\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>day</th>\n",
              "      <th>Country</th>\n",
              "      <th>City</th>\n",
              "      <th>lat</th>\n",
              "      <th>lon</th>\n",
              "      <th>co_cnt</th>\n",
              "      <th>co_min</th>\n",
              "      <th>...</th>\n",
              "      <th>rank</th>\n",
              "      <th>Population</th>\n",
              "      <th>Prev</th>\n",
              "      <th>Growth</th>\n",
              "      <th>kfold</th>\n",
              "      <th>lon_cos</th>\n",
              "      <th>lon_sin</th>\n",
              "      <th>month_cos</th>\n",
              "      <th>month_sin</th>\n",
              "      <th>month_world</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>-27.46794</td>\n",
              "      <td>153.02809</td>\n",
              "      <td>38</td>\n",
              "      <td>0.749</td>\n",
              "      <td>...</td>\n",
              "      <td>201.0</td>\n",
              "      <td>2472222.0</td>\n",
              "      <td>2439467.0</td>\n",
              "      <td>0.0134</td>\n",
              "      <td>0</td>\n",
              "      <td>0.891229</td>\n",
              "      <td>-0.453554</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>54</td>\n",
              "      <td>-12.46113</td>\n",
              "      <td>130.84185</td>\n",
              "      <td>47</td>\n",
              "      <td>2.594</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0.653973</td>\n",
              "      <td>-0.756518</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>146</td>\n",
              "      <td>-37.81400</td>\n",
              "      <td>144.96332</td>\n",
              "      <td>17</td>\n",
              "      <td>1.190</td>\n",
              "      <td>...</td>\n",
              "      <td>79.0</td>\n",
              "      <td>5150766.0</td>\n",
              "      <td>5061439.0</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>2</td>\n",
              "      <td>0.818785</td>\n",
              "      <td>-0.574101</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>176</td>\n",
              "      <td>-32.92953</td>\n",
              "      <td>151.78010</td>\n",
              "      <td>63</td>\n",
              "      <td>4.586</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.881139</td>\n",
              "      <td>-0.472857</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>193</td>\n",
              "      <td>-31.95224</td>\n",
              "      <td>115.86140</td>\n",
              "      <td>47</td>\n",
              "      <td>4.689</td>\n",
              "      <td>...</td>\n",
              "      <td>242.0</td>\n",
              "      <td>2092649.0</td>\n",
              "      <td>2067333.0</td>\n",
              "      <td>0.0122</td>\n",
              "      <td>1</td>\n",
              "      <td>0.436196</td>\n",
              "      <td>-0.899852</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>278</td>\n",
              "      <td>-34.42400</td>\n",
              "      <td>150.89345</td>\n",
              "      <td>179</td>\n",
              "      <td>4.554</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.873717</td>\n",
              "      <td>-0.486435</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>51.22047</td>\n",
              "      <td>4.40026</td>\n",
              "      <td>93</td>\n",
              "      <td>4.011</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.997052</td>\n",
              "      <td>-0.076724</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>50.41136</td>\n",
              "      <td>4.44448</td>\n",
              "      <td>11</td>\n",
              "      <td>0.096</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.996993</td>\n",
              "      <td>-0.077493</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>137</td>\n",
              "      <td>50.63373</td>\n",
              "      <td>5.56749</td>\n",
              "      <td>12</td>\n",
              "      <td>0.097</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.995283</td>\n",
              "      <td>-0.097018</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>247</td>\n",
              "      <td>-23.54750</td>\n",
              "      <td>-46.63611</td>\n",
              "      <td>354</td>\n",
              "      <td>1.048</td>\n",
              "      <td>...</td>\n",
              "      <td>5.0</td>\n",
              "      <td>22429800.0</td>\n",
              "      <td>22237472.0</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.686629</td>\n",
              "      <td>0.727008</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>275</td>\n",
              "      <td>-20.31944</td>\n",
              "      <td>-40.33778</td>\n",
              "      <td>29</td>\n",
              "      <td>0.117</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.762242</td>\n",
              "      <td>0.647293</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>272</td>\n",
              "      <td>49.24966</td>\n",
              "      <td>-123.11934</td>\n",
              "      <td>34</td>\n",
              "      <td>0.110</td>\n",
              "      <td>...</td>\n",
              "      <td>191.0</td>\n",
              "      <td>2631690.0</td>\n",
              "      <td>2606351.0</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>3</td>\n",
              "      <td>0.546385</td>\n",
              "      <td>0.837534</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>31</td>\n",
              "      <td>-22.45667</td>\n",
              "      <td>-68.92371</td>\n",
              "      <td>22</td>\n",
              "      <td>2.320</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.359611</td>\n",
              "      <td>0.933102</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>208</td>\n",
              "      <td>-33.04752</td>\n",
              "      <td>-71.44249</td>\n",
              "      <td>28</td>\n",
              "      <td>0.078</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.318256</td>\n",
              "      <td>0.948005</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>255</td>\n",
              "      <td>-35.42640</td>\n",
              "      <td>-71.65542</td>\n",
              "      <td>241</td>\n",
              "      <td>0.058</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.314731</td>\n",
              "      <td>0.949181</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>15</td>\n",
              "      <td>39.90750</td>\n",
              "      <td>116.39723</td>\n",
              "      <td>199</td>\n",
              "      <td>5.209</td>\n",
              "      <td>...</td>\n",
              "      <td>8.0</td>\n",
              "      <td>21333332.0</td>\n",
              "      <td>20896820.0</td>\n",
              "      <td>0.0209</td>\n",
              "      <td>4</td>\n",
              "      <td>0.444592</td>\n",
              "      <td>-0.895733</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>35</td>\n",
              "      <td>28.19874</td>\n",
              "      <td>112.97087</td>\n",
              "      <td>47</td>\n",
              "      <td>0.073</td>\n",
              "      <td>...</td>\n",
              "      <td>84.0</td>\n",
              "      <td>4809887.0</td>\n",
              "      <td>4694722.0</td>\n",
              "      <td>0.0245</td>\n",
              "      <td>2</td>\n",
              "      <td>0.390263</td>\n",
              "      <td>-0.920703</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>40</td>\n",
              "      <td>30.66667</td>\n",
              "      <td>104.06667</td>\n",
              "      <td>336</td>\n",
              "      <td>5.015</td>\n",
              "      <td>...</td>\n",
              "      <td>36.0</td>\n",
              "      <td>9478521.0</td>\n",
              "      <td>9305116.0</td>\n",
              "      <td>0.0186</td>\n",
              "      <td>3</td>\n",
              "      <td>0.243051</td>\n",
              "      <td>-0.970014</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>65</td>\n",
              "      <td>23.02677</td>\n",
              "      <td>113.13148</td>\n",
              "      <td>162</td>\n",
              "      <td>2.152</td>\n",
              "      <td>...</td>\n",
              "      <td>54.0</td>\n",
              "      <td>7497263.0</td>\n",
              "      <td>7406751.0</td>\n",
              "      <td>0.0122</td>\n",
              "      <td>1</td>\n",
              "      <td>0.392842</td>\n",
              "      <td>-0.919606</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>2019</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>68</td>\n",
              "      <td>41.88669</td>\n",
              "      <td>123.94363</td>\n",
              "      <td>103</td>\n",
              "      <td>2.543</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>0.558377</td>\n",
              "      <td>-0.829587</td>\n",
              "      <td>0.866025</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20 rows × 64 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8face887-a3dd-4fbb-8dac-af6ee7c48755')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8face887-a3dd-4fbb-8dac-af6ee7c48755 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8face887-a3dd-4fbb-8dac-af6ee7c48755');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numericals = Numericals()\n",
        "# 数字に関する変換\n",
        "# train_df_num = numericals.transform(train_df)\n",
        "\n",
        "# ラベルに関する変換\n",
        "# labelencoding = LabelEncoding(columns = [])\n",
        "\n",
        "# labelencoding.fit(train_df_num)\n",
        "# train_df_labeled = labelencoding.transform(train_df_num)"
      ],
      "metadata": {
        "id": "90PpUjny14tH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractTreeModel:\n",
        "    def __init__(self, prediction_type=\"regression\"):\n",
        "        self.model = None\n",
        "        self.prediction_type = prediction_type\n",
        "\n",
        "    def train(self,\n",
        "              params: dict,\n",
        "              X_train: pd.DataFrame,\n",
        "              y_train: np.ndarray,\n",
        "              X_val: pd.DataFrame,\n",
        "              y_val: np.ndarray,\n",
        "              train_weights: Optional[np.ndarray] = None,\n",
        "              val_weights: Optional[np.ndarray] = None,\n",
        "              train_params: Optional[dict] = None):\n",
        "        if train_params is None:\n",
        "            train_params = {}\n",
        "\n",
        "        model = self._train(\n",
        "            params,\n",
        "            X_train, y_train,\n",
        "            X_val, y_val,\n",
        "            train_weights, val_weights,\n",
        "            train_params)\n",
        "        self.model = model\n",
        "        return self\n",
        "\n",
        "    def _train(self,\n",
        "               params,\n",
        "               X_train,\n",
        "               y_train,\n",
        "               X_val,\n",
        "               y_val,\n",
        "               train_weights,\n",
        "               val_weights,\n",
        "               train_params):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def feature_names_(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _check_if_trained(self):\n",
        "        assert self.model is not None, \"You need to train the model first\"\n",
        "\n",
        "\n",
        "class LGBModel(AbstractTreeModel):\n",
        "    def _train(self,\n",
        "               params,\n",
        "               X_train, y_train,\n",
        "               X_val, y_val,\n",
        "               train_weights, val_weights,\n",
        "               train_params):\n",
        "        trn_data = lgb.Dataset(X_train, y_train, weight=train_weights)\n",
        "        val_data = lgb.Dataset(X_val, y_val, weight=val_weights)\n",
        "        model = lgb.train(params=params,\n",
        "                          train_set=trn_data,\n",
        "                          valid_sets=[trn_data, val_data],\n",
        "                          **train_params,)\n",
        "                        #   verbose_eval=False)\n",
        "        return model\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> np.ndarray:\n",
        "        self._check_if_trained()\n",
        "        return self.model.predict(X, num_iteration=self.model.best_iteration)\n",
        "\n",
        "    @property\n",
        "    def feature_names_(self):\n",
        "        self._check_if_trained()\n",
        "        return self.model.feature_name()\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        self._check_if_trained()\n",
        "        return self.model.feature_importance(importance_type=\"gain\")\n",
        "\n",
        "\n",
        "def get_tree_model(name: str) -> Type[AbstractTreeModel]:\n",
        "    DEFINED_MODELS = {\n",
        "        \"lgb\": LGBModel,\n",
        "        # 実装していない\n",
        "        # \"xgb\": XGBModel,\n",
        "        # \"cat\": CatBoostModel\n",
        "    }\n",
        "    model = DEFINED_MODELS.get(name)\n",
        "    if model is None:\n",
        "        raise ValueError(\n",
        "            \"\"\"Invalid model name: {}.\n",
        "            Pre-defined model names are as follows: {}\"\"\".format(\n",
        "                name,\n",
        "                \",\".join(DEFINED_MODELS.keys())\n",
        "            ))\n",
        "    return model"
      ],
      "metadata": {
        "id": "6YsXccMiCXvd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def viz_feature_importances(feature_importances: pd.DataFrame)-> None:\n",
        "    fi_gby = feature_importances.groupby(\"feature\").agg({\n",
        "        \"importance\": [\"mean\", \"std\"]\n",
        "    }).sort_values((\"importance\", \"mean\"), ascending=0).index\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=fi_gby)\n",
        "\n",
        "    plt.title('LightGBM Features (avg over folds)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # plt.savefig('lgbm_importances-01.png')"
      ],
      "metadata": {
        "id": "70dJPb20ISl_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {\n",
        "    \"num_boost_round\": 10000,\n",
        "    \"callbacks\": [\n",
        "                  lgb.early_stopping(stopping_rounds= 200, verbose=True),\n",
        "                  lgb.log_evaluation(period = 500)\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "55k-amEm_9Xb"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_tree_model(\"lgb\")()\n",
        "\n",
        "num_cols = [c for c in train_df_.columns if train_df_[c].dtype != np.object]\n",
        "if \"id\" in num_cols:\n",
        "    num_cols.remove(\"id\")\n",
        "if \"kfold\" in num_cols:\n",
        "    num_cols.remove(\"kfold\")\n",
        "num_cols.remove(\"pm25_mid\")\n",
        "\n",
        "target_cols = \"pm25_mid\""
      ],
      "metadata": {
        "id": "gm7PQWKwWS7W"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cols = [\n",
        "'year',\n",
        "'month', \n",
        "# 'day', \n",
        "'lat', 'lon',\n",
        " 'co_cnt', 'co_min', 'co_mid', 'co_max', 'co_var', \n",
        " 'o3_cnt', 'o3_min', 'o3_mid', 'o3_max', 'o3_var', \n",
        " 'so2_cnt', 'so2_min', 'so2_mid', 'so2_max', 'so2_var', 'no2_cnt', \n",
        " 'no2_min', 'no2_mid', 'no2_max', 'no2_var',\n",
        "'temperature_cnt', 'temperature_min', 'temperature_mid', 'temperature_max', 'temperature_var', \n",
        "'humidity_cnt', 'humidity_min', 'humidity_mid', 'humidity_max', 'humidity_var', \n",
        "'pressure_cnt', 'pressure_min', 'pressure_mid', 'pressure_max', 'pressure_var', \n",
        "'ws_cnt', 'ws_min', 'ws_mid', 'ws_max', 'ws_var', \n",
        "'dew_cnt', 'dew_min', 'dew_mid', 'dew_max', 'dew_var', \n",
        "'rank', 'Population', 'Prev', 'Growth', 'lon_cos', 'lon_sin', 'month_cos', 'month_sin', 'month_world'\n",
        "] + cat_cols"
      ],
      "metadata": {
        "id": "4zuVyajikQhk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = 1\n",
        "train_data = train_df_[train_df_[\"kfold\"] != i].reset_index(drop = True)\n",
        "valid_data = train_df_[train_df_[\"kfold\"] == i].reset_index(drop = True)\n",
        "\n",
        "lgb_train = lgb.Dataset(train_data[use_cols], train_data[target_cols])\n",
        "lgb_eval = lgb.Dataset(valid_data[use_cols], valid_data[target_cols])\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    params= {\n",
        "            'boosting': 'gbdt',\n",
        "            'objective': 'rmse',\n",
        "            'metric': 'rmse',\n",
        "            'learning_rate': trial.suggest_uniform('learning_rate', 0.001, 0.1),\n",
        "            \"max_depth\": trial.suggest_categorical(\"max_depth\", [8, 16, 32]),\n",
        "            \"num_leaves\": trial.suggest_categorical(\"num_leaves\", [8, 16, 32, 64]),\n",
        "            'subsample': 0.7,\n",
        "            'subsample_freq': 1,\n",
        "            \"min_data_in_leaf\":30, \n",
        "            \"device\": \"gpu\",  \n",
        "            \"seed\":2022,\n",
        "    }\n",
        "\n",
        "    gbm = lgb.train(params, lgb_train, valid_sets=lgb_eval, **train_params)\n",
        "    preds = gbm.predict(valid_data[use_cols])\n",
        "\n",
        "    accuracy = np.sqrt(mean_squared_error(valid_data[target_cols], preds))\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "sIcqSSNHv4s7"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction = \"minimize\")\n",
        "study.optimize(objective, n_trials = 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecr7FVcIv4vv",
        "outputId": "a4125cf4-1637-4fae-e36c-b90d86b29884"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 12:54:55,796]\u001b[0m A new study created in memory with name: no-name-e28b8a48-bed7-4b4f-9b89-63604cb11219\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.012300 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.3979\n",
            "[1000]\tvalid_0's rmse: 21.2946\n",
            "Early stopping, best iteration is:\n",
            "[1239]\tvalid_0's rmse: 21.2692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 12:55:07,776]\u001b[0m Trial 0 finished with value: 21.26924628191411 and parameters: {'learning_rate': 0.06251589821700465, 'max_depth': 32, 'num_leaves': 16}. Best is trial 0 with value: 21.26924628191411.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.008150 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 24.9367\n",
            "[1000]\tvalid_0's rmse: 22.8769\n",
            "[1500]\tvalid_0's rmse: 22.1897\n",
            "[2000]\tvalid_0's rmse: 21.8681\n",
            "[2500]\tvalid_0's rmse: 21.6584\n",
            "[3000]\tvalid_0's rmse: 21.5106\n",
            "[3500]\tvalid_0's rmse: 21.4124\n",
            "[4000]\tvalid_0's rmse: 21.338\n",
            "[4500]\tvalid_0's rmse: 21.2813\n",
            "[5000]\tvalid_0's rmse: 21.2435\n",
            "[5500]\tvalid_0's rmse: 21.2138\n",
            "[6000]\tvalid_0's rmse: 21.1925\n",
            "[6500]\tvalid_0's rmse: 21.1732\n",
            "[7000]\tvalid_0's rmse: 21.1573\n",
            "[7500]\tvalid_0's rmse: 21.1433\n",
            "[8000]\tvalid_0's rmse: 21.1307\n",
            "[8500]\tvalid_0's rmse: 21.1201\n",
            "[9000]\tvalid_0's rmse: 21.115\n",
            "[9500]\tvalid_0's rmse: 21.1046\n",
            "[10000]\tvalid_0's rmse: 21.0987\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[9998]\tvalid_0's rmse: 21.0987\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 12:58:35,993]\u001b[0m Trial 1 finished with value: 21.09865653847205 and parameters: {'learning_rate': 0.00202375133369951, 'max_depth': 16, 'num_leaves': 64}. Best is trial 1 with value: 21.09865653847205.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007792 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.3074\n",
            "Early stopping, best iteration is:\n",
            "[619]\tvalid_0's rmse: 21.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 12:58:44,930]\u001b[0m Trial 2 finished with value: 21.299978558013866 and parameters: {'learning_rate': 0.08316989406730703, 'max_depth': 32, 'num_leaves': 32}. Best is trial 1 with value: 21.09865653847205.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.008029 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[281]\tvalid_0's rmse: 21.4594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 12:58:50,334]\u001b[0m Trial 3 finished with value: 21.459438478621802 and parameters: {'learning_rate': 0.09752154455827944, 'max_depth': 16, 'num_leaves': 32}. Best is trial 1 with value: 21.09865653847205.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.012666 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.6283\n",
            "[1000]\tvalid_0's rmse: 21.3309\n",
            "[1500]\tvalid_0's rmse: 21.2629\n",
            "[2000]\tvalid_0's rmse: 21.2088\n",
            "[2500]\tvalid_0's rmse: 21.2\n",
            "[3000]\tvalid_0's rmse: 21.1746\n",
            "Early stopping, best iteration is:\n",
            "[3209]\tvalid_0's rmse: 21.1646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 12:59:29,965]\u001b[0m Trial 4 finished with value: 21.164561500196488 and parameters: {'learning_rate': 0.01782760406034501, 'max_depth': 16, 'num_leaves': 32}. Best is trial 1 with value: 21.09865653847205.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007936 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "Early stopping, best iteration is:\n",
            "[215]\tvalid_0's rmse: 21.2986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 12:59:37,314]\u001b[0m Trial 5 finished with value: 21.29855066969297 and parameters: {'learning_rate': 0.08578852796881131, 'max_depth': 32, 'num_leaves': 64}. Best is trial 1 with value: 21.09865653847205.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007821 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.9129\n",
            "[1000]\tvalid_0's rmse: 21.5226\n",
            "[1500]\tvalid_0's rmse: 21.3864\n",
            "[2000]\tvalid_0's rmse: 21.3283\n",
            "[2500]\tvalid_0's rmse: 21.287\n",
            "[3000]\tvalid_0's rmse: 21.2634\n",
            "Early stopping, best iteration is:\n",
            "[3054]\tvalid_0's rmse: 21.257\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:00:04,630]\u001b[0m Trial 6 finished with value: 21.257033309298397 and parameters: {'learning_rate': 0.020372830835990303, 'max_depth': 8, 'num_leaves': 16}. Best is trial 1 with value: 21.09865653847205.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007651 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.9816\n",
            "[1000]\tvalid_0's rmse: 21.4101\n",
            "[1500]\tvalid_0's rmse: 21.2431\n",
            "[2000]\tvalid_0's rmse: 21.1789\n",
            "[2500]\tvalid_0's rmse: 21.142\n",
            "[3000]\tvalid_0's rmse: 21.1147\n",
            "[3500]\tvalid_0's rmse: 21.0985\n",
            "Early stopping, best iteration is:\n",
            "[3645]\tvalid_0's rmse: 21.0961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:01:19,279]\u001b[0m Trial 7 finished with value: 21.096056681586532 and parameters: {'learning_rate': 0.007309021087733992, 'max_depth': 16, 'num_leaves': 64}. Best is trial 7 with value: 21.096056681586532.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007807 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.3975\n",
            "[1000]\tvalid_0's rmse: 21.3312\n",
            "Early stopping, best iteration is:\n",
            "[999]\tvalid_0's rmse: 21.331\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:01:28,730]\u001b[0m Trial 8 finished with value: 21.330955824657007 and parameters: {'learning_rate': 0.07315589085079434, 'max_depth': 8, 'num_leaves': 16}. Best is trial 7 with value: 21.096056681586532.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007718 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 22.6522\n",
            "[1000]\tvalid_0's rmse: 21.9525\n",
            "[1500]\tvalid_0's rmse: 21.689\n",
            "[2000]\tvalid_0's rmse: 21.5634\n",
            "[2500]\tvalid_0's rmse: 21.4711\n",
            "[3000]\tvalid_0's rmse: 21.4149\n",
            "[3500]\tvalid_0's rmse: 21.3701\n",
            "[4000]\tvalid_0's rmse: 21.3375\n",
            "[4500]\tvalid_0's rmse: 21.3125\n",
            "[5000]\tvalid_0's rmse: 21.2927\n",
            "[5500]\tvalid_0's rmse: 21.2852\n",
            "[6000]\tvalid_0's rmse: 21.2754\n",
            "[6500]\tvalid_0's rmse: 21.2695\n",
            "Early stopping, best iteration is:\n",
            "[6308]\tvalid_0's rmse: 21.2689\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:02:24,025]\u001b[0m Trial 9 finished with value: 21.26887438962333 and parameters: {'learning_rate': 0.00947374034058138, 'max_depth': 8, 'num_leaves': 16}. Best is trial 7 with value: 21.096056681586532.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007579 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 22.0063\n",
            "[1000]\tvalid_0's rmse: 21.6085\n",
            "[1500]\tvalid_0's rmse: 21.4647\n",
            "[2000]\tvalid_0's rmse: 21.4039\n",
            "[2500]\tvalid_0's rmse: 21.3571\n",
            "[3000]\tvalid_0's rmse: 21.333\n",
            "[3500]\tvalid_0's rmse: 21.3038\n",
            "Early stopping, best iteration is:\n",
            "[3394]\tvalid_0's rmse: 21.2948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:02:46,290]\u001b[0m Trial 10 finished with value: 21.29482712809557 and parameters: {'learning_rate': 0.03964973941998728, 'max_depth': 16, 'num_leaves': 8}. Best is trial 7 with value: 21.096056681586532.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.008773 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.0911\n",
            "Early stopping, best iteration is:\n",
            "[629]\tvalid_0's rmse: 21.0641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:03:01,818]\u001b[0m Trial 11 finished with value: 21.06409970188268 and parameters: {'learning_rate': 0.03610474734378254, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.008209 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.1783\n",
            "Early stopping, best iteration is:\n",
            "[484]\tvalid_0's rmse: 21.1708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:03:14,570]\u001b[0m Trial 12 finished with value: 21.17076224648513 and parameters: {'learning_rate': 0.038805099898794826, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007528 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.1909\n",
            "[1000]\tvalid_0's rmse: 21.1325\n",
            "Early stopping, best iteration is:\n",
            "[901]\tvalid_0's rmse: 21.1302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:03:34,661]\u001b[0m Trial 13 finished with value: 21.13017425245341 and parameters: {'learning_rate': 0.031763813759346865, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.012252 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.2298\n",
            "Early stopping, best iteration is:\n",
            "[523]\tvalid_0's rmse: 21.2133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:03:47,569]\u001b[0m Trial 14 finished with value: 21.213320958452897 and parameters: {'learning_rate': 0.05368394310227474, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007777 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 22.3212\n",
            "[1000]\tvalid_0's rmse: 21.8511\n",
            "[1500]\tvalid_0's rmse: 21.6293\n",
            "[2000]\tvalid_0's rmse: 21.4931\n",
            "[2500]\tvalid_0's rmse: 21.4257\n",
            "[3000]\tvalid_0's rmse: 21.3849\n",
            "[3500]\tvalid_0's rmse: 21.3443\n",
            "Early stopping, best iteration is:\n",
            "[3395]\tvalid_0's rmse: 21.3416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:04:10,410]\u001b[0m Trial 15 finished with value: 21.341576760800134 and parameters: {'learning_rate': 0.026375973323377933, 'max_depth': 16, 'num_leaves': 8}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007691 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.3828\n",
            "Early stopping, best iteration is:\n",
            "[523]\tvalid_0's rmse: 21.3677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:04:23,326]\u001b[0m Trial 16 finished with value: 21.367685194946382 and parameters: {'learning_rate': 0.05011965210743135, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.008278 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 27.8766\n",
            "[1000]\tvalid_0's rmse: 24.8636\n",
            "[1500]\tvalid_0's rmse: 23.5254\n",
            "[2000]\tvalid_0's rmse: 22.8393\n",
            "[2500]\tvalid_0's rmse: 22.4252\n",
            "[3000]\tvalid_0's rmse: 22.1662\n",
            "[3500]\tvalid_0's rmse: 21.9893\n",
            "[4000]\tvalid_0's rmse: 21.8541\n",
            "[4500]\tvalid_0's rmse: 21.7413\n",
            "[5000]\tvalid_0's rmse: 21.645\n",
            "[5500]\tvalid_0's rmse: 21.5663\n",
            "[6000]\tvalid_0's rmse: 21.4997\n",
            "[6500]\tvalid_0's rmse: 21.4466\n",
            "[7000]\tvalid_0's rmse: 21.4012\n",
            "[7500]\tvalid_0's rmse: 21.3632\n",
            "[8000]\tvalid_0's rmse: 21.3316\n",
            "[8500]\tvalid_0's rmse: 21.3052\n",
            "[9000]\tvalid_0's rmse: 21.2796\n",
            "[9500]\tvalid_0's rmse: 21.2608\n",
            "[10000]\tvalid_0's rmse: 21.2408\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[9998]\tvalid_0's rmse: 21.2407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:08:15,620]\u001b[0m Trial 17 finished with value: 21.240733785957072 and parameters: {'learning_rate': 0.0010304868024737955, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007519 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.5331\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[1000]\tvalid_0's rmse: 21.2387\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[1500]\tvalid_0's rmse: 21.1757\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[2000]\tvalid_0's rmse: 21.1661\n",
            "Early stopping, best iteration is:\n",
            "[1823]\tvalid_0's rmse: 21.1598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:08:54,661]\u001b[0m Trial 18 finished with value: 21.15976373729886 and parameters: {'learning_rate': 0.01270005105803481, 'max_depth': 8, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007857 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.934\n",
            "[1000]\tvalid_0's rmse: 21.5734\n",
            "[1500]\tvalid_0's rmse: 21.4867\n",
            "[2000]\tvalid_0's rmse: 21.4266\n",
            "Early stopping, best iteration is:\n",
            "[1915]\tvalid_0's rmse: 21.4087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:09:08,026]\u001b[0m Trial 19 finished with value: 21.40870575495391 and parameters: {'learning_rate': 0.04504667415044112, 'max_depth': 32, 'num_leaves': 8}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007800 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.174\n",
            "Early stopping, best iteration is:\n",
            "[761]\tvalid_0's rmse: 21.1128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:09:26,347]\u001b[0m Trial 20 finished with value: 21.11277670981082 and parameters: {'learning_rate': 0.02910040506631278, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007678 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 23.421\n",
            "[1000]\tvalid_0's rmse: 22.114\n",
            "[1500]\tvalid_0's rmse: 21.698\n",
            "[2000]\tvalid_0's rmse: 21.4692\n",
            "[2500]\tvalid_0's rmse: 21.3332\n",
            "[3000]\tvalid_0's rmse: 21.2517\n",
            "[3500]\tvalid_0's rmse: 21.2052\n",
            "[4000]\tvalid_0's rmse: 21.169\n",
            "[4500]\tvalid_0's rmse: 21.1414\n",
            "[5000]\tvalid_0's rmse: 21.1202\n",
            "[5500]\tvalid_0's rmse: 21.1069\n",
            "[6000]\tvalid_0's rmse: 21.0925\n",
            "[6500]\tvalid_0's rmse: 21.0838\n",
            "[7000]\tvalid_0's rmse: 21.078\n",
            "[7500]\tvalid_0's rmse: 21.0736\n",
            "[8000]\tvalid_0's rmse: 21.0695\n",
            "Early stopping, best iteration is:\n",
            "[8134]\tvalid_0's rmse: 21.0662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:12:07,741]\u001b[0m Trial 21 finished with value: 21.066214263726557 and parameters: {'learning_rate': 0.003234090566851679, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007686 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.7485\n",
            "[1000]\tvalid_0's rmse: 21.2996\n",
            "[1500]\tvalid_0's rmse: 21.1922\n",
            "[2000]\tvalid_0's rmse: 21.1537\n",
            "[2500]\tvalid_0's rmse: 21.1325\n",
            "[3000]\tvalid_0's rmse: 21.1214\n",
            "[3500]\tvalid_0's rmse: 21.1054\n",
            "[4000]\tvalid_0's rmse: 21.0988\n",
            "[4500]\tvalid_0's rmse: 21.0936\n",
            "Early stopping, best iteration is:\n",
            "[4442]\tvalid_0's rmse: 21.0908\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:13:29,523]\u001b[0m Trial 22 finished with value: 21.090790173377272 and parameters: {'learning_rate': 0.009514948318552024, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.008214 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.2592\n",
            "[1000]\tvalid_0's rmse: 21.1099\n",
            "Early stopping, best iteration is:\n",
            "[1117]\tvalid_0's rmse: 21.0991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:13:54,378]\u001b[0m Trial 23 finished with value: 21.099101223478236 and parameters: {'learning_rate': 0.021212913264422064, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007941 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.4198\n",
            "[1000]\tvalid_0's rmse: 21.1538\n",
            "[1500]\tvalid_0's rmse: 21.0963\n",
            "[2000]\tvalid_0's rmse: 21.0722\n",
            "Early stopping, best iteration is:\n",
            "[1922]\tvalid_0's rmse: 21.0703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:14:34,515]\u001b[0m Trial 24 finished with value: 21.070267055510257 and parameters: {'learning_rate': 0.014054453018578753, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.008074 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.1954\n",
            "Early stopping, best iteration is:\n",
            "[649]\tvalid_0's rmse: 21.1804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:14:50,374]\u001b[0m Trial 25 finished with value: 21.180399524936387 and parameters: {'learning_rate': 0.034913353708312486, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007525 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.3445\n",
            "[1000]\tvalid_0's rmse: 21.1385\n",
            "Early stopping, best iteration is:\n",
            "[1266]\tvalid_0's rmse: 21.1042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:15:19,199]\u001b[0m Trial 26 finished with value: 21.104237023501806 and parameters: {'learning_rate': 0.017096481448807557, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007388 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.4359\n",
            "[1000]\tvalid_0's rmse: 21.2491\n",
            "[1500]\tvalid_0's rmse: 21.2181\n",
            "Early stopping, best iteration is:\n",
            "[1678]\tvalid_0's rmse: 21.1993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:15:41,383]\u001b[0m Trial 27 finished with value: 21.19927895345008 and parameters: {'learning_rate': 0.02588262666660538, 'max_depth': 16, 'num_leaves': 32}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007841 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.8842\n",
            "[1000]\tvalid_0's rmse: 21.5537\n",
            "[1500]\tvalid_0's rmse: 21.4554\n",
            "Early stopping, best iteration is:\n",
            "[1732]\tvalid_0's rmse: 21.4226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:15:53,123]\u001b[0m Trial 28 finished with value: 21.42256761049742 and parameters: {'learning_rate': 0.06080615453197049, 'max_depth': 32, 'num_leaves': 8}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007698 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 22.1733\n",
            "[1000]\tvalid_0's rmse: 21.6623\n",
            "[1500]\tvalid_0's rmse: 21.4776\n",
            "[2000]\tvalid_0's rmse: 21.3736\n",
            "[2500]\tvalid_0's rmse: 21.3282\n",
            "[3000]\tvalid_0's rmse: 21.2961\n",
            "[3500]\tvalid_0's rmse: 21.2708\n",
            "[4000]\tvalid_0's rmse: 21.2515\n",
            "[4500]\tvalid_0's rmse: 21.245\n",
            "Early stopping, best iteration is:\n",
            "[4338]\tvalid_0's rmse: 21.241\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:16:30,480]\u001b[0m Trial 29 finished with value: 21.240960737744768 and parameters: {'learning_rate': 0.014821065310326502, 'max_depth': 8, 'num_leaves': 16}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007980 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 22.5106\n",
            "[1000]\tvalid_0's rmse: 21.693\n",
            "[1500]\tvalid_0's rmse: 21.3964\n",
            "[2000]\tvalid_0's rmse: 21.268\n",
            "[2500]\tvalid_0's rmse: 21.1921\n",
            "[3000]\tvalid_0's rmse: 21.142\n",
            "[3500]\tvalid_0's rmse: 21.1233\n",
            "[4000]\tvalid_0's rmse: 21.1025\n",
            "[4500]\tvalid_0's rmse: 21.0955\n",
            "Early stopping, best iteration is:\n",
            "[4345]\tvalid_0's rmse: 21.094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:18:00,705]\u001b[0m Trial 30 finished with value: 21.094043012291408 and parameters: {'learning_rate': 0.00492720984153789, 'max_depth': 32, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.012103 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.8014\n",
            "[1000]\tvalid_0's rmse: 21.3057\n",
            "[1500]\tvalid_0's rmse: 21.1854\n",
            "[2000]\tvalid_0's rmse: 21.1337\n",
            "[2500]\tvalid_0's rmse: 21.1133\n",
            "[3000]\tvalid_0's rmse: 21.0956\n",
            "[3500]\tvalid_0's rmse: 21.0754\n",
            "Early stopping, best iteration is:\n",
            "[3435]\tvalid_0's rmse: 21.075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:19:08,563]\u001b[0m Trial 31 finished with value: 21.07501463016563 and parameters: {'learning_rate': 0.008766290092249909, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.010586 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 24.5591\n",
            "[1000]\tvalid_0's rmse: 22.6797\n",
            "[1500]\tvalid_0's rmse: 22.0714\n",
            "[2000]\tvalid_0's rmse: 21.7854\n",
            "[2500]\tvalid_0's rmse: 21.5836\n",
            "[3000]\tvalid_0's rmse: 21.4494\n",
            "[3500]\tvalid_0's rmse: 21.3627\n",
            "[4000]\tvalid_0's rmse: 21.3039\n",
            "[4500]\tvalid_0's rmse: 21.2544\n",
            "[5000]\tvalid_0's rmse: 21.2202\n",
            "[5500]\tvalid_0's rmse: 21.1943\n",
            "[6000]\tvalid_0's rmse: 21.1721\n",
            "[6500]\tvalid_0's rmse: 21.1548\n",
            "[7000]\tvalid_0's rmse: 21.1391\n",
            "[7500]\tvalid_0's rmse: 21.1263\n",
            "[8000]\tvalid_0's rmse: 21.1156\n",
            "[8500]\tvalid_0's rmse: 21.1086\n",
            "[9000]\tvalid_0's rmse: 21.1044\n",
            "[9500]\tvalid_0's rmse: 21.0987\n",
            "[10000]\tvalid_0's rmse: 21.0942\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[9991]\tvalid_0's rmse: 21.094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:22:27,651]\u001b[0m Trial 32 finished with value: 21.094038739395216 and parameters: {'learning_rate': 0.002235114244076862, 'max_depth': 16, 'num_leaves': 64}. Best is trial 11 with value: 21.06409970188268.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007437 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.1901\n",
            "[1000]\tvalid_0's rmse: 21.0619\n",
            "Early stopping, best iteration is:\n",
            "[1070]\tvalid_0's rmse: 21.0546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:22:51,440]\u001b[0m Trial 33 finished with value: 21.054599271507584 and parameters: {'learning_rate': 0.022683360798373735, 'max_depth': 16, 'num_leaves': 64}. Best is trial 33 with value: 21.054599271507584.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.011232 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.1661\n",
            "[1000]\tvalid_0's rmse: 21.0629\n",
            "Early stopping, best iteration is:\n",
            "[1287]\tvalid_0's rmse: 21.0516\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:23:18,013]\u001b[0m Trial 34 finished with value: 21.051582599514024 and parameters: {'learning_rate': 0.026406281478069466, 'max_depth': 16, 'num_leaves': 64}. Best is trial 34 with value: 21.051582599514024.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007807 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.3369\n",
            "[1000]\tvalid_0's rmse: 21.224\n",
            "Early stopping, best iteration is:\n",
            "[1229]\tvalid_0's rmse: 21.2035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:23:34,288]\u001b[0m Trial 35 finished with value: 21.203509012502092 and parameters: {'learning_rate': 0.0425628185370528, 'max_depth': 16, 'num_leaves': 32}. Best is trial 34 with value: 21.051582599514024.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007433 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.2564\n",
            "[1000]\tvalid_0's rmse: 21.1609\n",
            "[1500]\tvalid_0's rmse: 21.1408\n",
            "Early stopping, best iteration is:\n",
            "[1471]\tvalid_0's rmse: 21.1329\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:24:04,553]\u001b[0m Trial 36 finished with value: 21.132884617368227 and parameters: {'learning_rate': 0.023092248419491385, 'max_depth': 16, 'num_leaves': 64}. Best is trial 34 with value: 21.051582599514024.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007518 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.3386\n",
            "[1000]\tvalid_0's rmse: 21.2244\n",
            "[1500]\tvalid_0's rmse: 21.1967\n",
            "Early stopping, best iteration is:\n",
            "[1567]\tvalid_0's rmse: 21.188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:24:25,014]\u001b[0m Trial 37 finished with value: 21.18799492032906 and parameters: {'learning_rate': 0.03308414279168931, 'max_depth': 32, 'num_leaves': 32}. Best is trial 34 with value: 21.051582599514024.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007905 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.2714\n",
            "Early stopping, best iteration is:\n",
            "[648]\tvalid_0's rmse: 21.2591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:24:39,872]\u001b[0m Trial 38 finished with value: 21.25912099647053 and parameters: {'learning_rate': 0.04882971280293125, 'max_depth': 16, 'num_leaves': 64}. Best is trial 34 with value: 21.051582599514024.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007963 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.2743\n",
            "Early stopping, best iteration is:\n",
            "[624]\tvalid_0's rmse: 21.2559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:24:54,107]\u001b[0m Trial 39 finished with value: 21.255888342207147 and parameters: {'learning_rate': 0.0563904787943764, 'max_depth': 16, 'num_leaves': 64}. Best is trial 34 with value: 21.051582599514024.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.008047 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.9139\n",
            "[1000]\tvalid_0's rmse: 21.5027\n",
            "[1500]\tvalid_0's rmse: 21.3398\n",
            "[2000]\tvalid_0's rmse: 21.267\n",
            "[2500]\tvalid_0's rmse: 21.2392\n",
            "[3000]\tvalid_0's rmse: 21.2223\n",
            "[3500]\tvalid_0's rmse: 21.2133\n",
            "Early stopping, best iteration is:\n",
            "[3766]\tvalid_0's rmse: 21.2005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:25:26,301]\u001b[0m Trial 40 finished with value: 21.200486982163113 and parameters: {'learning_rate': 0.020357881946167514, 'max_depth': 8, 'num_leaves': 16}. Best is trial 34 with value: 21.051582599514024.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007581 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.3404\n",
            "[1000]\tvalid_0's rmse: 21.1341\n",
            "[1500]\tvalid_0's rmse: 21.1035\n",
            "Early stopping, best iteration is:\n",
            "[1470]\tvalid_0's rmse: 21.1005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:25:58,063]\u001b[0m Trial 41 finished with value: 21.10051804971697 and parameters: {'learning_rate': 0.016704266687052896, 'max_depth': 16, 'num_leaves': 64}. Best is trial 34 with value: 21.051582599514024.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007705 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.4779\n",
            "[1000]\tvalid_0's rmse: 21.1688\n",
            "[1500]\tvalid_0's rmse: 21.0993\n",
            "[2000]\tvalid_0's rmse: 21.0647\n",
            "[2500]\tvalid_0's rmse: 21.0455\n",
            "[3000]\tvalid_0's rmse: 21.0232\n",
            "[3500]\tvalid_0's rmse: 21.0176\n",
            "Early stopping, best iteration is:\n",
            "[3503]\tvalid_0's rmse: 21.0173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:27:02,990]\u001b[0m Trial 42 finished with value: 21.017326176587435 and parameters: {'learning_rate': 0.012555278381634567, 'max_depth': 16, 'num_leaves': 64}. Best is trial 42 with value: 21.017326176587435.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007279 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.2824\n",
            "[1000]\tvalid_0's rmse: 21.2329\n",
            "Early stopping, best iteration is:\n",
            "[1041]\tvalid_0's rmse: 21.227\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:27:24,777]\u001b[0m Trial 43 finished with value: 21.22698130769275 and parameters: {'learning_rate': 0.03732831341470372, 'max_depth': 16, 'num_leaves': 64}. Best is trial 42 with value: 21.017326176587435.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007542 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.1696\n",
            "[1000]\tvalid_0's rmse: 21.0838\n",
            "[1500]\tvalid_0's rmse: 21.0666\n",
            "Early stopping, best iteration is:\n",
            "[1614]\tvalid_0's rmse: 21.0629\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:27:56,935]\u001b[0m Trial 44 finished with value: 21.062875101280326 and parameters: {'learning_rate': 0.02603219000205154, 'max_depth': 16, 'num_leaves': 64}. Best is trial 42 with value: 21.017326176587435.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007974 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.1439\n",
            "[1000]\tvalid_0's rmse: 21.0959\n",
            "Early stopping, best iteration is:\n",
            "[1124]\tvalid_0's rmse: 21.0856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:28:20,704]\u001b[0m Trial 45 finished with value: 21.085584814370836 and parameters: {'learning_rate': 0.029452671987212423, 'max_depth': 16, 'num_leaves': 64}. Best is trial 42 with value: 21.017326176587435.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007874 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.7882\n",
            "[1000]\tvalid_0's rmse: 21.459\n",
            "[1500]\tvalid_0's rmse: 21.3416\n",
            "[2000]\tvalid_0's rmse: 21.272\n",
            "[2500]\tvalid_0's rmse: 21.2544\n",
            "Early stopping, best iteration is:\n",
            "[2702]\tvalid_0's rmse: 21.2382\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:28:44,633]\u001b[0m Trial 46 finished with value: 21.238174874198155 and parameters: {'learning_rate': 0.02464623770178221, 'max_depth': 16, 'num_leaves': 16}. Best is trial 42 with value: 21.017326176587435.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007581 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.6809\n",
            "Early stopping, best iteration is:\n",
            "[657]\tvalid_0's rmse: 21.5709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:28:49,804]\u001b[0m Trial 47 finished with value: 21.57093744290167 and parameters: {'learning_rate': 0.09510066489346752, 'max_depth': 16, 'num_leaves': 8}. Best is trial 42 with value: 21.017326176587435.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007431 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[500]\tvalid_0's rmse: 21.4102\n",
            "Early stopping, best iteration is:\n",
            "[335]\tvalid_0's rmse: 21.3618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:28:58,731]\u001b[0m Trial 48 finished with value: 21.36178565569087 and parameters: {'learning_rate': 0.06960520551192806, 'max_depth': 8, 'num_leaves': 64}. Best is trial 42 with value: 21.017326176587435.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007727 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\tvalid_0's rmse: 21.3422\n",
            "[1000]\tvalid_0's rmse: 21.1909\n",
            "Early stopping, best iteration is:\n",
            "[1132]\tvalid_0's rmse: 21.1768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-04-19 13:29:14,539]\u001b[0m Trial 49 finished with value: 21.17675800111147 and parameters: {'learning_rate': 0.030142478359765402, 'max_depth': 16, 'num_leaves': 32}. Best is trial 42 with value: 21.017326176587435.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(study.best_params, study.best_trial, study.best_trials, study.best_value)\n",
        "study.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_36NJX3Cuesq",
        "outputId": "23cb87d7-15f0-44cc-fa3f-fd80a8a44e58"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'learning_rate': 0.012555278381634567, 'max_depth': 16, 'num_leaves': 64} FrozenTrial(number=42, values=[21.017326176587435], datetime_start=datetime.datetime(2022, 4, 19, 13, 25, 58, 65232), datetime_complete=datetime.datetime(2022, 4, 19, 13, 27, 2, 989814), params={'learning_rate': 0.012555278381634567, 'max_depth': 16, 'num_leaves': 64}, distributions={'learning_rate': UniformDistribution(high=0.1, low=0.001), 'max_depth': CategoricalDistribution(choices=(8, 16, 32)), 'num_leaves': CategoricalDistribution(choices=(8, 16, 32, 64))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=42, state=TrialState.COMPLETE, value=None) [FrozenTrial(number=42, values=[21.017326176587435], datetime_start=datetime.datetime(2022, 4, 19, 13, 25, 58, 65232), datetime_complete=datetime.datetime(2022, 4, 19, 13, 27, 2, 989814), params={'learning_rate': 0.012555278381634567, 'max_depth': 16, 'num_leaves': 64}, distributions={'learning_rate': UniformDistribution(high=0.1, low=0.001), 'max_depth': CategoricalDistribution(choices=(8, 16, 32)), 'num_leaves': CategoricalDistribution(choices=(8, 16, 32, 64))}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=42, state=TrialState.COMPLETE, value=None)] 21.017326176587435\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.012555278381634567, 'max_depth': 16, 'num_leaves': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params= {\n",
        "        'boosting': 'gbdt',\n",
        "        'objective': 'rmse',\n",
        "        'metric': 'rmse',\n",
        "        'learning_rate': study.best_params[\"learning_rate\"],\n",
        "        \"max_depth\": study.best_params[\"max_depth\"],\n",
        "        \"num_leaves\": study.best_params[\"num_leaves\"],\n",
        "        'subsample': 0.7,\n",
        "        'subsample_freq': 1,\n",
        "        \"min_data_in_leaf\":30, \n",
        "        \"device\": \"gpu\",  \n",
        "        \"seed\":2022,\n",
        "}"
      ],
      "metadata": {
        "id": "O7gneSfCue0V"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = []\n",
        "\n",
        "oof_df = train_df_[[\"id\"]]\n",
        "oof_df[\"oof\"] = -1\n",
        "\n",
        "from lightgbm import early_stopping\n",
        "from lightgbm import log_evaluation\n",
        "\n",
        "for i in range(NUM_FOLDS):\n",
        "    train_data = train_df_[train_df_[\"kfold\"] != i].reset_index(drop = True)\n",
        "    valid_data = train_df_[train_df_[\"kfold\"] == i].reset_index(drop = True)\n",
        "\n",
        "    print(f\"train_data nums: {len(train_data)}, valid_data nums: {len(valid_data)}\")\n",
        "\n",
        "    model.train(\n",
        "        params,\n",
        "        train_params = train_params,\n",
        "        X_train = train_data[use_cols],\n",
        "        y_train = train_data[target_cols],\n",
        "        X_val = valid_data[use_cols],\n",
        "        y_val = valid_data[target_cols],\n",
        "    )\n",
        "    models.append(model)\n",
        "\n",
        "    feature_importance = pd.DataFrame()\n",
        "    feature_importance[\"feature\"] = model.feature_names_\n",
        "    feature_importance[\"importance\"] = model.feature_importances_\n",
        "    feature_importance[\"fold\"] = i\n",
        "\n",
        "    oof = model.predict(\n",
        "        valid_data[use_cols]\n",
        "    )\n",
        "    oof_df.loc[train_df[\"kfold\"] == i, \"oof\"] = oof\n",
        "\n",
        "train_oof = train_df.merge(oof_df[[\"id\", \"oof\"]], on = \"id\", how = \"left\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7MEvpyAW-s_",
        "outputId": "a315dbcc-7802-4385-9192-84511c2c4c97"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_data nums: 156755, valid_data nums: 39186\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12927\n",
            "[LightGBM] [Info] Number of data points in the train set: 156755, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.008042 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 58.695189\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\ttraining's rmse: 19.2941\tvalid_1's rmse: 25.0635\n",
            "[1000]\ttraining's rmse: 18.0418\tvalid_1's rmse: 24.7778\n",
            "[1500]\ttraining's rmse: 17.2151\tvalid_1's rmse: 24.6921\n",
            "[2000]\ttraining's rmse: 16.5452\tvalid_1's rmse: 24.6278\n",
            "[2500]\ttraining's rmse: 15.9591\tvalid_1's rmse: 24.6099\n",
            "Early stopping, best iteration is:\n",
            "[2368]\ttraining's rmse: 16.1074\tvalid_1's rmse: 24.5983\n",
            "train_data nums: 156758, valid_data nums: 39183\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12906\n",
            "[LightGBM] [Info] Number of data points in the train set: 156758, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007494 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.317361\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\ttraining's rmse: 19.4916\tvalid_1's rmse: 21.4976\n",
            "[1000]\ttraining's rmse: 18.2141\tvalid_1's rmse: 21.1813\n",
            "[1500]\ttraining's rmse: 17.3711\tvalid_1's rmse: 21.1055\n",
            "[2000]\ttraining's rmse: 16.6848\tvalid_1's rmse: 21.0805\n",
            "Early stopping, best iteration is:\n",
            "[1920]\ttraining's rmse: 16.7878\tvalid_1's rmse: 21.078\n",
            "train_data nums: 156760, valid_data nums: 39181\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12934\n",
            "[LightGBM] [Info] Number of data points in the train set: 156760, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007416 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 60.259055\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\ttraining's rmse: 19.4546\tvalid_1's rmse: 21.983\n",
            "[1000]\ttraining's rmse: 18.1833\tvalid_1's rmse: 21.7314\n",
            "[1500]\ttraining's rmse: 17.3481\tvalid_1's rmse: 21.6316\n",
            "[2000]\ttraining's rmse: 16.6665\tvalid_1's rmse: 21.585\n",
            "[2500]\ttraining's rmse: 16.0785\tvalid_1's rmse: 21.5606\n",
            "[3000]\ttraining's rmse: 15.5497\tvalid_1's rmse: 21.5488\n",
            "[3500]\ttraining's rmse: 15.0706\tvalid_1's rmse: 21.5353\n",
            "Early stopping, best iteration is:\n",
            "[3525]\ttraining's rmse: 15.0465\tvalid_1's rmse: 21.5322\n",
            "train_data nums: 156730, valid_data nums: 39211\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12935\n",
            "[LightGBM] [Info] Number of data points in the train set: 156730, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007662 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 59.072621\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\ttraining's rmse: 19.2694\tvalid_1's rmse: 22.2298\n",
            "[1000]\ttraining's rmse: 17.9893\tvalid_1's rmse: 21.92\n",
            "[1500]\ttraining's rmse: 17.1519\tvalid_1's rmse: 21.8228\n",
            "[2000]\ttraining's rmse: 16.4784\tvalid_1's rmse: 21.7606\n",
            "[2500]\ttraining's rmse: 15.8893\tvalid_1's rmse: 21.7304\n",
            "[3000]\ttraining's rmse: 15.3628\tvalid_1's rmse: 21.7011\n",
            "[3500]\ttraining's rmse: 14.878\tvalid_1's rmse: 21.684\n",
            "[4000]\ttraining's rmse: 14.4293\tvalid_1's rmse: 21.6683\n",
            "[4500]\ttraining's rmse: 14.014\tvalid_1's rmse: 21.6615\n",
            "[5000]\ttraining's rmse: 13.6231\tvalid_1's rmse: 21.6535\n",
            "Early stopping, best iteration is:\n",
            "[4993]\ttraining's rmse: 13.6282\tvalid_1's rmse: 21.653\n",
            "train_data nums: 156761, valid_data nums: 39180\n",
            "[LightGBM] [Info] This is the GPU trainer!!\n",
            "[LightGBM] [Info] Total Bins 12932\n",
            "[LightGBM] [Info] Number of data points in the train set: 156761, number of used features: 60\n",
            "[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n",
            "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
            "[LightGBM] [Info] GPU programs have been built\n",
            "[LightGBM] [Info] Size of histogram bin entry: 8\n",
            "[LightGBM] [Info] 60 dense feature groups (8.97 MB) transferred to GPU in 0.007574 secs. 0 sparse feature groups\n",
            "[LightGBM] [Info] Start training from score 58.656422\n",
            "Training until validation scores don't improve for 200 rounds\n",
            "[500]\ttraining's rmse: 19.2255\tvalid_1's rmse: 22.3624\n",
            "[1000]\ttraining's rmse: 17.9526\tvalid_1's rmse: 22.0574\n",
            "[1500]\ttraining's rmse: 17.1261\tvalid_1's rmse: 21.9483\n",
            "[2000]\ttraining's rmse: 16.4609\tvalid_1's rmse: 21.8874\n",
            "[2500]\ttraining's rmse: 15.8688\tvalid_1's rmse: 21.8522\n",
            "[3000]\ttraining's rmse: 15.3444\tvalid_1's rmse: 21.8228\n",
            "[3500]\ttraining's rmse: 14.8639\tvalid_1's rmse: 21.8009\n",
            "[4000]\ttraining's rmse: 14.4206\tvalid_1's rmse: 21.7825\n",
            "[4500]\ttraining's rmse: 14.0023\tvalid_1's rmse: 21.7625\n",
            "Early stopping, best iteration is:\n",
            "[4714]\ttraining's rmse: 13.8326\tvalid_1's rmse: 21.7566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "viz_feature_importances(feature_importance)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "SCZb-hXEIIkr",
        "outputId": "5cc79c4d-ad5e-4fb0-cb89-9daca80780f7"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oof_score = np.sqrt(mean_squared_error(train_oof[\"pm25_mid\"], train_oof[\"oof\"]))\n",
        "oof_score = format(oof_score, \".3f\")\n",
        "print(f\"oof score: {oof_score}\")\n",
        "\n",
        "# 22.332\n",
        "# 22.333 now"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIAV6MGrJ1IF",
        "outputId": "284fba11-eec8-43f8-8912-b51af7e68723"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oof score: 22.159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_oof[[\"id\",\"pm25_mid\", \"kfold\", \"oof\"]].to_csv(os.path.join(OOF_DIR, \"oof.csv\"), index= False)"
      ],
      "metadata": {
        "id": "eAWRkjDaaIta"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# predict"
      ],
      "metadata": {
        "id": "8vAXJVnWSySV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_predict = None\n",
        "for model in models:\n",
        "    predict = model.predict(\n",
        "        test_df_[use_cols],\n",
        "    )\n",
        "    if sum_predict is None:\n",
        "        sum_predict = predict\n",
        "    else:\n",
        "        sum_predict += predict\n",
        "\n",
        "mean_predict = sum_predict / len(models)"
      ],
      "metadata": {
        "id": "p1434DZVHg0W"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mean_predict))\n",
        "print(sample_df.shape)\n",
        "print(test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjcrS25rrje9",
        "outputId": "3896458e-7b56-487f-f037-84f9005bcd52"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "53509\n",
            "(53509, 2)\n",
            "(53509, 57)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df.loc[:,\"predict\"] = mean_predict"
      ],
      "metadata": {
        "id": "xot64X_yHg4L"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kVae9iW6cL0D",
        "outputId": "53248e51-e646-4a74-b6c2-e52895452724"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       id     predict\n",
              "0  195942   32.219262\n",
              "1  195943   34.120460\n",
              "2  195944   24.159254\n",
              "3  195945   62.709874\n",
              "4  195946  137.972062"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4eef2542-8e07-4608-9da7-741cde9268ba\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>predict</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>195942</td>\n",
              "      <td>32.219262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>195943</td>\n",
              "      <td>34.120460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>195944</td>\n",
              "      <td>24.159254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>195945</td>\n",
              "      <td>62.709874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>195946</td>\n",
              "      <td>137.972062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4eef2542-8e07-4608-9da7-741cde9268ba')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4eef2542-8e07-4608-9da7-741cde9268ba button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4eef2542-8e07-4608-9da7-741cde9268ba');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_df.to_csv(os.path.join(SUB_DIR, \"submission.csv\"), index= False, header = False)"
      ],
      "metadata": {
        "id": "87NkPMdTSrmH"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# submit"
      ],
      "metadata": {
        "id": "ZO0WqelkHyfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install signate > /dev/null\n",
        "!mkdir /root/.signate\n",
        "!cp /content/drive/MyDrive/signate/signate.json /root/.signate/signate.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhTe6h6yDH8X",
        "outputId": "7a336727-1881-4b28-a3ca-d8a5d2d131f6"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_file = os.path.join(SUB_DIR, \"submission.csv\")"
      ],
      "metadata": {
        "id": "oSmQPSHLTX6E"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment = f\"cv:{oof_score}_\" + NOTEBOOK_NAME\n",
        "comment"
      ],
      "metadata": {
        "id": "8yM6uPondJnm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c6c7fb6d-6501-421c-ae03-9a295a3ae471"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cv:22.159_FP009-008-tuning'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit"
      ],
      "metadata": {
        "id": "LvoIkzVdibh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!signate submit --competition-id=624 {submission_file} --note {comment}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69-UNJ1YTuej",
        "outputId": "a75c1e96-f1b9-472f-bd8d-d196d3089446"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mYou have successfully submitted your predictions.We will send you the submission result to your email address.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://signate.jp/competitions/624/submissions"
      ],
      "metadata": {
        "id": "fdianTtHED1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RrKqaQNXq6HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MVDD0781EEep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}